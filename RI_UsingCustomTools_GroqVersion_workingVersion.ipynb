{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aJp9VMsmijKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858663a6-491d-4684-e79b-5b209e5d5fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install required libraries for our project\n",
        "!pip install --quiet langchain langgraph langchain-groq \"langchain_core>=0.1.28\" python-dotenv pandas\n",
        "\n",
        "# Import necessary modules\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from dotenv import load_dotenv\n",
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "\n",
        "# LangChain and LangGraph components\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage\n",
        "from langchain_groq import ChatGroq\n",
        "from langgraph.graph import StateGraph, END"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, math"
      ],
      "metadata": {
        "id": "7R7NOWBOts8O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import re\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from textwrap import dedent\n",
        "from langchain_core.tools import tool\n",
        "from typing import Optional, List\n",
        "from groq import Groq\n",
        "\n",
        "# Cell 2: Mount Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load environment variables from the .env file in your Drive\n",
        "env_path = '/content/drive/My Drive/env1.env'\n",
        "load_dotenv(dotenv_path=env_path)"
      ],
      "metadata": {
        "id": "gpspunVD2I4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c68d05-e66d-4fd8-a573-fd8471992552"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access your API keys\n",
        "groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "if groq_api_key:\n",
        "    print(\"✅ Groq API Key loaded successfully.\")\n",
        "else:\n",
        "    print(\"❌ Groq API Key not found. Please check your env1.env file.\")\n",
        "\n",
        "geoapify_api_key = os.environ.get(\"Geoapify_API_Key\")\n",
        "print(\"Geoapify API Key loaded:\", \"Yes\" if geoapify_api_key else \"No\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qye25_F-wbVd",
        "outputId": "cd7828c9-3524-43f0-b786-465ecd3a2e9c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Groq API Key loaded successfully.\n",
            "Geoapify API Key loaded: Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CJzZkNFiyhP",
        "outputId": "2ed12772-1e41-45be-861d-0b8de426613f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Redfin data loaded successfully. Shape: (8873849, 21)\n",
            "✅ Data preparation complete. Columns have been selected, cleaned, and renamed.\n",
            "Final available columns: ['region_type', 'region_id', 'period_end', 'zip_code', 'property_type', 'median_sale_price', 'median_sale_price_mom', 'median_sale_price_yoy', 'median_list_price', 'median_list_price_mom', 'median_list_price_yoy', 'median_ppsf', 'median_ppsf_mom', 'median_ppsf_yoy', 'homes_sold', 'homes_sold_mom', 'homes_sold_yoy', 'median_dom', 'median_dom_mom', 'median_dom_yoy', 'avg_sale_to_list']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# --- Load and Prepare Data ---\n",
        "# Load the Redfin dataset from your Drive\n",
        "file_path = '/content/drive/My Drive/redfin.csv'\n",
        "try:\n",
        "    redfindf = pd.read_csv(file_path)\n",
        "    print(f\"✅ Redfin data loaded successfully. Shape: {redfindf.shape}\")\n",
        "\n",
        "    # Keep only the columns we need for analysis\n",
        "    columns_to_keep = [\n",
        "    'region_type_id','table_id','period_end', 'region', 'property_type', 'median_sale_price',\n",
        "    'median_sale_price_mom', 'median_sale_price_yoy', 'median_list_price',\n",
        "    'median_list_price_mom', 'median_list_price_yoy', 'median_ppsf',\n",
        "    'median_ppsf_mom', 'median_ppsf_yoy', 'homes_sold', 'homes_sold_mom',\n",
        "    'homes_sold_yoy', 'median_dom' ,'median_dom_mom' ,'median_dom_yoy' ,'avg_sale_to_list']\n",
        "    data_subset = redfindf[columns_to_keep].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "    # --- STEP 2: Clean and Rename columns based on your logic ---\n",
        "    # Clean the 'region' column BEFORE renaming it\n",
        "    data_subset['region'] = data_subset['region'].str.replace('Zip Code: ', '')\n",
        "\n",
        "    # Define the rename mapping\n",
        "    rename_dict = {\n",
        "        'table_id': 'region_id',\n",
        "        'region_type_id': 'region_type',\n",
        "        'region': 'zip_code'\n",
        "    }\n",
        "\n",
        "    # Rename the columns to their final, standardized names\n",
        "    data_subset = data_subset.rename(columns=rename_dict)\n",
        "\n",
        "    # --- STEP 3: Perform final data type conversions ---\n",
        "    data_subset['period_end'] = pd.to_datetime(data_subset['period_end'])\n",
        "\n",
        "    print(\"✅ Data preparation complete. Columns have been selected, cleaned, and renamed.\")\n",
        "    print(\"Final available columns:\", list(data_subset.columns))\n",
        "\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"❌ Error: The file was not found at {file_path}. Please check the path.\")\n",
        "    data_subset = None\n",
        "except ValueError as e:\n",
        "    print(f\"❌ Data Error: {e}\")\n",
        "    data_subset = None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Cell 2.1: Initialize the Groq language model\n",
        "model = ChatGroq(temperature=0.2, model_name=\"openai/gpt-oss-120b\", groq_api_key=groq_api_key) #llama-3.3-70b-versatile ; llama3-70b-8192\n",
        "'''"
      ],
      "metadata": {
        "id": "sMyzATyp2RW8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "472ac5c0-605c-484d-a75a-29456adda504"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Cell 2.1: Initialize the Groq language model\\nmodel = ChatGroq(temperature=0.2, model_name=\"openai/gpt-oss-120b\", groq_api_key=groq_api_key) #llama-3.3-70b-versatile ; llama3-70b-8192\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2.2: define globals\n",
        "CMA_RESULTS: list[dict] = []\n",
        "\n",
        "# ---- Scraping caches ----\n",
        "_listings_cache: dict[tuple, tuple[pd.DataFrame, float]] = {}\n",
        "\n",
        "# ---- Geocode caches ----\n",
        "# Subject geocode cache: address -> (lat, lon, formatted, postcode, ts)\n",
        "_subject_geocode_cache: dict[str, tuple[float, float, str | None, str | None, float]] = {}\n",
        "\n",
        "# Comps geocode cache: (mode, zip, min_k, max_k, pages) -> (DataFrame_with_lat_lon, ts)\n",
        "_geocoded_comps_cache: dict[tuple, tuple[pd.DataFrame, float]] = {}\n",
        "\n"
      ],
      "metadata": {
        "id": "TCJKurPD7dqS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.0: helper functions\n",
        "# Helper Functions for use throughout\n",
        "\n",
        "def extract_tsv_data(text_blob: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Parses a string containing TSV (tab-separated values) data and cleans it into a DataFrame.\n",
        "    This function is designed to handle the specific output format from the Groq LLM.\n",
        "    \"\"\"\n",
        "    # Pattern to match the TSV block, skipping any preamble text from the LLM.\n",
        "    tsv_pattern = re.compile(r'(?:(?:[^\\n]*\\n)*?)^([^\\n]+\\t[^\\n]*(?:\\n[^\\n]+\\t[^\\n]*)*)$', re.MULTILINE)\n",
        "    tsv_match = tsv_pattern.search(text_blob)\n",
        "\n",
        "    if tsv_match:\n",
        "        tsv_data = tsv_match.group(1)\n",
        "        df = pd.read_csv(io.StringIO(tsv_data), sep='\\t', header=None)\n",
        "\n",
        "        # Ensure the dataframe has exactly 5 columns before renaming\n",
        "        if df.shape[1] != 5:\n",
        "            # If not, return an empty dataframe as the data is malformed\n",
        "            return pd.DataFrame(columns=[\"address\", \"price\", \"sq ft\", \"bedrooms\", \"bathrooms\"])\n",
        "\n",
        "        df.columns = [\"address\", \"price\", \"sq ft\", \"bedrooms\", \"bathrooms\"]\n",
        "\n",
        "        # --- Data Cleaning Logic (from your original code) ---\n",
        "        df['sq ft'] = df['sq ft'].astype(str).str.replace(r'[, ]*(sq ft|sqft|SF|ft²)$', '', regex=True, case=False).str.replace(',', '', regex=True)\n",
        "        df['price'] = df['price'].astype(str).str.replace(r'[\\$,]', '', regex=True)\n",
        "        df['bedrooms'] = df['bedrooms'].astype(str).str.replace(r'\\s*beds$', '', regex=True, case=False)\n",
        "        df['bathrooms'] = df['bathrooms'].astype(str).str.replace(r'\\s*baths$', '', regex=True, case=False)\n",
        "\n",
        "        # Convert to numeric, coercing errors to NaN\n",
        "        for col in ['sq ft', 'price', 'bedrooms', 'bathrooms']:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Drop any row where a conversion failed or data was missing\n",
        "        df.dropna(inplace=True)\n",
        "\n",
        "        return df\n",
        "    else:\n",
        "        # If no TSV data is found in the LLM's response\n",
        "        return pd.DataFrame(columns=[\"address\", \"price\", \"sq ft\", \"bedrooms\", \"bathrooms\"])\n",
        "\n",
        "\n",
        "\n",
        "def extract_tsv_data_rent(text_blob: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Parses a string containing TSV (tab-separated values) data and cleans it into a DataFrame.\n",
        "    \"\"\"\n",
        "    tsv_pattern = re.compile(r'(?:(?:[^\\n]*\\n)*?)^([^\\n]+\\t[^\\n]*(?:\\n[^\\n]+\\t[^\\n]*)*)$', re.MULTILINE)\n",
        "    tsv_match = tsv_pattern.search(text_blob)\n",
        "\n",
        "    if tsv_match:\n",
        "        tsv_data = tsv_match.group(1)\n",
        "        # Use StringIO to handle the string as a file\n",
        "        df = pd.read_csv(io.StringIO(tsv_data), sep='\\t', header=None)\n",
        "\n",
        "        if df.shape[1] != 5:\n",
        "            return pd.DataFrame(columns=[\"address\", \"rent\", \"sq ft\", \"bedrooms\", \"bathrooms\"])\n",
        "\n",
        "        df.columns = [\"address\", \"rent\", \"sq ft\", \"bedrooms\", \"bathrooms\"]\n",
        "\n",
        "        # --- Data Cleaning Logic ---\n",
        "        df['sq ft'] = df['sq ft'].astype(str).str.replace(r'[, ]*(sq ft|sqft|SF|ft²)$', '', regex=True, case=False).str.replace(',', '', regex=True)\n",
        "        df['rent'] = df['rent'].astype(str).str.replace(r'[\\$,]', '', regex=True)\n",
        "        df['bedrooms'] = df['bedrooms'].astype(str).str.replace(r'\\s*beds$', '', regex=True, case=False)\n",
        "        df['bathrooms'] = df['bathrooms'].astype(str).str.replace(r'\\s*baths$', '', regex=True, case=False)\n",
        "\n",
        "        for col in ['sq ft', 'rent', 'bedrooms', 'bathrooms']:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        df.dropna(inplace=True)\n",
        "        return df\n",
        "    else:\n",
        "        return pd.DataFrame(columns=[\"address\", \"rent\", \"sq ft\", \"bedrooms\", \"bathrooms\"])\n",
        "\n",
        "\n",
        "# ===== Shared Redfin scraper helper =====\n",
        "from typing import Callable, Optional\n",
        "import time, requests, pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from textwrap import dedent\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "# One cache for everything: key -> (DataFrame, timestamp)\n",
        "#_listings_cache: dict[tuple, tuple[pd.DataFrame, float]] = {}\n",
        "\n",
        "def _default_prompt(mode: str, zip_code: str, min_price: Optional[int], max_price: Optional[int]) -> str:\n",
        "    if mode == \"sale\":\n",
        "        return dedent(f\"\"\"\n",
        "            Review this report for homes in {zip_code} with prices between ${min_price}k and ${max_price}k.\n",
        "            Your job is to list all the homes in the report in a TSV (tab-separated) with columns:\n",
        "            \"address\", \"price\", \"sq ft\", \"bedrooms\", \"bathrooms\".\n",
        "            The report format repeats the address at the end of each block; always use that final address.\n",
        "            Ignore any orphan records. Provide ONLY the TSV data.\n",
        "        \"\"\")\n",
        "    else:  # rent\n",
        "        return dedent(f\"\"\"\n",
        "            Review this report for homes for rent in {zip_code}.\n",
        "            Output TSV with columns: \"address\", \"rent\", \"sq ft\", \"bedrooms\", \"bathrooms\".\n",
        "            The address appears at the end of each block; use that final address.\n",
        "            If a stat is blank, put 0. Ignore orphan records. Provide ONLY the TSV data.\n",
        "        \"\"\")\n",
        "\n",
        "def _default_extract_fn(mode: str):\n",
        "    # Use your existing extractors\n",
        "    return extract_tsv_data if mode == \"sale\" else extract_tsv_data_rent\n",
        "\n",
        "def _page_url_fn(mode: str, zip_code: str, min_price: Optional[int], max_price: Optional[int]) -> Callable[[int], str]:\n",
        "    if mode == \"sale\":\n",
        "        base = f\"https://www.redfin.com/zipcode/{zip_code}/filter/min-price={min_price}k,max-price={max_price}k\"\n",
        "        return lambda page: base if page == 1 else f\"{base}/page-{page}\"\n",
        "    else:  # rent\n",
        "        base = f\"https://www.redfin.com/zipcode/{zip_code}/apartments-for-rent/filter/property-type=house+condo+townhouse\"\n",
        "        return lambda page: base if page == 1 else f\"{base}/page-{page}\"\n",
        "\n",
        "def scrape_redfin_to_df(\n",
        "    *,\n",
        "    mode: str,                     # \"sale\" | \"rent\"\n",
        "    zip_code: str,\n",
        "    max_pages_to_scrape: int,\n",
        "    min_price: Optional[int] = None,   # thousands (sale only)\n",
        "    max_price: Optional[int] = None,   # thousands (sale only)\n",
        "    cache_ttl_seconds: Optional[int] = None,  # None = never expires\n",
        "    llm_client: Optional[Groq] = None,\n",
        "    llm_model_name: str = \"llama-3.3-70b-versatile\",\n",
        "    prompt: Optional[str] = None,\n",
        "    extract_fn: Optional[Callable[[str], pd.DataFrame]] = None,\n",
        ") -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Scrape + parse via LLM + concat + cache; returns final DataFrame or None.\"\"\"\n",
        "    assert mode in (\"sale\", \"rent\")\n",
        "    cache_key = (mode, zip_code, min_price, max_price, max_pages_to_scrape)\n",
        "\n",
        "    # cache\n",
        "    if cache_key in _listings_cache:\n",
        "        df_cached, ts = _listings_cache[cache_key]\n",
        "        if cache_ttl_seconds is None or (time.time() - ts) < cache_ttl_seconds:\n",
        "            print(f\"[CACHE HIT] {cache_key} -> {len(df_cached)} rows\")\n",
        "            return df_cached\n",
        "        print(f\"[CACHE EXPIRED] {cache_key}\")\n",
        "\n",
        "    # defaults\n",
        "    if llm_client is None:\n",
        "        llm_client = Groq(api_key=groq_api_key)\n",
        "    if prompt is None:\n",
        "        prompt = _default_prompt(mode, zip_code, min_price, max_price)\n",
        "    if extract_fn is None:\n",
        "        extract_fn = _default_extract_fn(mode)\n",
        "    url_for_page = _page_url_fn(mode, zip_code, min_price, max_price)\n",
        "\n",
        "    all_dfs: list[pd.DataFrame] = []\n",
        "    for page in range(1, max_pages_to_scrape + 1):\n",
        "        url = url_for_page(page)\n",
        "        print(f\"Scraping page {page}: {url}\")\n",
        "        try:\n",
        "            resp = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"}, timeout=15)\n",
        "            resp.raise_for_status()\n",
        "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "            cards = soup.select(\"div.HomeCardContainer\")\n",
        "            if not cards:\n",
        "                print(f\"No home cards found on page {page}. Stopping scrape.\")\n",
        "                break\n",
        "\n",
        "            scraped_text = \"\\n\\n\".join(c.get_text(separator=\" \", strip=True) for c in cards)\n",
        "\n",
        "            # LLM parse → TSV\n",
        "            llm_response = llm_client.chat.completions.create(\n",
        "                model=llm_model_name,\n",
        "                messages=[\n",
        "                    {\"role\":\"system\",\"content\":prompt},\n",
        "                    {\"role\":\"user\",  \"content\":scraped_text},\n",
        "                ],\n",
        "                temperature=0.1,\n",
        "                max_tokens=4000,\n",
        "            )\n",
        "            extracted = llm_response.choices[0].message.content\n",
        "            page_df = extract_fn(extracted)\n",
        "\n",
        "            if not page_df.empty:\n",
        "                print(f\"Successfully extracted {len(page_df)} items from page {page}.\")\n",
        "                all_dfs.append(page_df)\n",
        "            else:\n",
        "                print(f\"Could not extract valid data from page {page}.\")\n",
        "\n",
        "            time.sleep(2)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching page {page}: {e}\")\n",
        "            break\n",
        "\n",
        "    if not all_dfs:\n",
        "        return None\n",
        "\n",
        "    final_df = pd.concat(all_dfs, ignore_index=True).drop_duplicates()\n",
        "    _listings_cache[cache_key] = (final_df, time.time())\n",
        "    print(f\"[SCRAPED & CACHED] {len(final_df)} rows for {cache_key}\")\n",
        "    return final_df\n",
        "\n",
        "\n",
        "# === CMA INCREMENTS ONLY: geo helpers, prompts, and the CMA tool ===\n",
        "import os, re, io, time, json, math, requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from textwrap import dedent\n",
        "from groq import Groq\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# ---------- Geo helpers (new) ----------\n",
        "\n",
        "def _haversine_km(lat1, lon1, lat2, lon2) -> float:\n",
        "    R = 6371.0\n",
        "    p1, p2 = math.radians(lat1), math.radians(lat2)\n",
        "    dlat = math.radians(lat2 - lat1)\n",
        "    dlon = math.radians(lon2 - lon1)\n",
        "    a = math.sin(dlat/2)**2 + math.cos(p1)*math.cos(p2)*math.sin(dlon/2)**2\n",
        "    return R * (2 * math.asin(math.sqrt(a)))\n",
        "\n",
        "def _geoapify_geocode(address: str, api_key: str, pause: float = 0.12):\n",
        "    \"\"\"Return (lat, lon, formatted, postcode) or (None, None, None, None).\"\"\"\n",
        "    if not address:\n",
        "        return None, None, None, None\n",
        "    url = (\n",
        "        \"https://api.geoapify.com/v1/geocode/search\"\n",
        "        f\"?text={requests.utils.quote(address)}&apiKey={api_key}\"\n",
        "    )\n",
        "    try:\n",
        "        resp = requests.get(url, timeout=12)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        feats = data.get(\"features\", [])\n",
        "        if not feats:\n",
        "            return None, None, None, None\n",
        "        props = feats[0].get(\"properties\", {})\n",
        "        lat, lon = props.get(\"lat\"), props.get(\"lon\")\n",
        "        fmt = props.get(\"formatted\")\n",
        "        pc  = props.get(\"postcode\")\n",
        "        time.sleep(pause)\n",
        "        return lat, lon, fmt, pc\n",
        "    except Exception:\n",
        "        return None, None, None, None\n",
        "\n",
        "def _zip_from_geocode(formatted: str | None, postcode: str | None) -> str | None:\n",
        "    if postcode and len(postcode) >= 5:\n",
        "        return postcode[:5]\n",
        "    if formatted:\n",
        "        m = re.search(r\"\\b(\\d{5})(?:-\\d{4})?\\b\", formatted)\n",
        "        if m:\n",
        "            return m.group(1)\n",
        "    return None\n",
        "\n",
        "def _geocode_df_addresses(df: pd.DataFrame, address_col: str, zip_hint: str,\n",
        "                          api_key: str, max_to_geocode: int = 250, sleep_sec: float = 0.12) -> pd.DataFrame:\n",
        "    \"\"\"Adds latitude/longitude columns by geocoding up to max_to_geocode rows.\"\"\"\n",
        "    work = df.copy()\n",
        "    if \"latitude\" not in work.columns:\n",
        "        work[\"latitude\"] = pd.NA\n",
        "    if \"longitude\" not in work.columns:\n",
        "        work[\"longitude\"] = pd.NA\n",
        "    work[\"full_address_for_geo\"] = work[address_col].astype(str) + f\", {zip_hint}\"\n",
        "    done = 0\n",
        "    for idx, row in work.iterrows():\n",
        "        if done >= max_to_geocode:\n",
        "            break\n",
        "        if pd.notna(row[\"latitude\"]) and pd.notna(row[\"longitude\"]):\n",
        "            continue\n",
        "        addr = row[\"full_address_for_geo\"]\n",
        "        lat, lon, _, _ = _geoapify_geocode(addr, api_key, pause=sleep_sec)\n",
        "        if lat is not None and lon is not None:\n",
        "            work.at[idx, \"latitude\"] = lat\n",
        "            work.at[idx, \"longitude\"] = lon\n",
        "            done += 1\n",
        "    return work\n",
        "\n",
        "\n",
        "def _select_nearest_by_distance_only(\n",
        "    df: pd.DataFrame,\n",
        "    max_comps: int,\n",
        "    kind: str,                  # \"rent\" or \"sale\"\n",
        "    min_distance_m: float = 0  # exclude anything within ~0 meters of subject\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Keep the nearest N rows by distance_km, excluding the subject itself\n",
        "    by removing rows whose distance is < min_distance_m.\n",
        "    \"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "\n",
        "    work = df.copy()\n",
        "    if \"distance_km\" not in work.columns:\n",
        "        print(\"[CMA] distance_km not found; returning input as-is.\")\n",
        "        return work.head(max_comps)\n",
        "\n",
        "    # Exclude the subject (or duplicate geocodes) by distance threshold\n",
        "    work = work[(work[\"distance_km\"] * 1000.0) >= min_distance_m]\n",
        "\n",
        "    # Pick nearest N\n",
        "    chosen = work.sort_values(\"distance_km\").head(max_comps)\n",
        "\n",
        "    # Quick peek so you can verify what goes to the LLM\n",
        "    cols = [\"address\", \"distance_km\", \"bedrooms\", \"bathrooms\", \"sq ft\"]\n",
        "    if kind == \"rent\" and \"rent\" in chosen.columns:\n",
        "        cols.insert(1, \"rent\")\n",
        "    if kind == \"sale\" and \"price\" in chosen.columns:\n",
        "        cols.insert(1, \"price\")\n",
        "\n",
        "    print(f\"[CMA] {kind.title()} comps (nearest-by-distance, top 5):\")\n",
        "    try:\n",
        "        print(chosen[cols].head(5).to_string(index=False))\n",
        "    except Exception:\n",
        "        print(chosen.head(5).to_string(index=False))\n",
        "\n",
        "    return chosen\n",
        "\n",
        "\n",
        "# ---------- Lightweight prompt renderer + default prompts (new) ----------\n",
        "\n",
        "def _render_prompt(template: str, ctx: dict) -> str:\n",
        "    try:\n",
        "        return template.format(**ctx)\n",
        "    except Exception:\n",
        "        # Fallback: append JSON blobs if formatting fails\n",
        "        suffix = \"\"\n",
        "        if \"closest_rental_neighbors_str\" in ctx:\n",
        "            suffix += \"\\n\\nNearest rental comps (JSON):\\n\" + ctx[\"closest_rental_neighbors_str\"]\n",
        "        if \"closest_forsale_neighbors_str\" in ctx:\n",
        "            suffix += \"\\n\\nNearest for-sale comps (JSON):\\n\" + ctx[\"closest_forsale_neighbors_str\"]\n",
        "        return template + suffix\n",
        "\n",
        "prompt_rent_estimate = dedent(f\"\"\" You are an excellent real estate agent, with a strong analytical mind.\n",
        "You have a client who want you to estimate the rent of a target property. You must use the target property's data on  bedrooms,  bathrooms and is area in sqft.\n",
        "Your research team has given you rental property data in JSON format of 15 nearby addresses. Each object has keys:\n",
        "address, rent, sq ft, bedrooms, bathrooms, latitude, longitude, distance_km.\n",
        "\n",
        "***Important:*** If any neighbor’s address exactly matches the target property address, ignore that entry—do not include it in your analysis or counts.\n",
        "***Important:*** Use brevity in your chain of thought.\n",
        "\n",
        "Also - these 15 properties are nearest to target property, therefore try and focus on characteristics of the target property and not the distance. Dont obsess only on bedroom or bathroom numbers. Similarly dont obsess only on sq ft. Have a balanced approach.\n",
        "Properties that are more similar to the target property will be the more immediate competition for the target property.\n",
        "Given this information and guidance estimate the rent of the target address. You can provide a range of rent estimates. But ensure that your range is not too broad. No more than plus or minus 5% of the target price.\n",
        "Finally,**clearly explain** in narrative form:\n",
        "- List comparables and Why you chose each comparable ***Important:*** you must try and find at least 4 closest comparable properties.\n",
        "- How the property features informed your final number\n",
        "\n",
        "\"\"\").strip()\n",
        "\n",
        "prompt_4sale_estimate = dedent(f\"\"\" You are an excellent real estate agent, with a strong analytical mind.\n",
        "You have a client who want you to estimate the rent of a target property. You must use the target property's data on  bedrooms,  bathrooms and is area in sqft.\n",
        "Your research team has given you rental property data in JSON format of 15 nearby addresses. Each object has keys:\n",
        "address, price, sq ft, bedrooms, bathrooms, latitude, longitude, distance_km.\n",
        "\n",
        "***Important:*** If any neighbor’s address exactly matches the target property address, ignore that entry—do not include it in your analysis.\n",
        "***Important:*** Use brevity in your chain of thought.\n",
        "\n",
        "Also - these properties are nearest to target property, therefore try and focus on characteristics of the target property and not the distance. Dont obsess only on bedroom or bathroom numbers. Similarly dont obsess only on sq ft. Have a balanced approach.\n",
        "For example if the target property has 2 bathrooms, and a nearby competiting property has 3 bathrooms with same bedrooms and nearly same sq ft, they should be comparable.\n",
        "Properties that are more similar to the target property will be the more immediate competition for the target property.\n",
        "Given this information and guidance estimate the selling price of the target address. You can provide a range of sale price estimates. But ensure that your range is not too broad. No more than plus or minus 5% of the target price.\n",
        "Finally, **clearly explain** in narrative form:\n",
        "- List comparables and Why you chose each comparable. ***Important:*** you must try and find at least 4 closest comparable properties.\n",
        "- How the property features informed your final number\n",
        "\n",
        "\"\"\").strip()\n",
        "\n",
        "def _infer_sale_band_k_from_llm(\n",
        "    address: str,\n",
        "    groq_key: str,\n",
        "    model: str = \"compound-beta\",\n",
        "    spread_k: int = 200,     # +$200k above floored min\n",
        "    floor_to_k: int = 100    # floor to nearest $100k\n",
        ") -> Optional[tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    Ask an LLM for a single-point price (USD) and convert it to a [min_k, max_k] band:\n",
        "      min_k = floor(price_k to nearest floor_to_k)\n",
        "      max_k = min_k + spread_k\n",
        "    Returns (min_k, max_k) in thousands, or None if parsing fails.\n",
        "    \"\"\"\n",
        "    client = Groq(api_key=groq_key)\n",
        "\n",
        "    system = (\n",
        "        \"You are a valuation assistant. \"\n",
        "        \"Return ONLY valid JSON like {\\\"price_usd\\\": 320000} representing your best single-point \"\n",
        "        \"estimate of the current market sale price of the given residential address. No prose.\"\n",
        "    )\n",
        "    user = f\"Address: {address}\\nReturn only JSON with 'price_usd'.\"\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
        "        temperature=0.1,\n",
        "        max_tokens=100,\n",
        "    )\n",
        "    txt = (resp.choices[0].message.content or \"\").strip()\n",
        "\n",
        "    price = None\n",
        "    # Try strict JSON first\n",
        "    try:\n",
        "        data = json.loads(txt)\n",
        "        price = data.get(\"price_usd\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Fallback: regex the largest dollar-like number\n",
        "    if not price:\n",
        "        m = re.search(r'(\\$?\\s?)(\\d{2,3}(?:[,\\s]\\d{3})+|\\d{5,})', txt)\n",
        "        if m:\n",
        "            s = m.group(2).replace(\",\", \"\").replace(\" \", \"\")\n",
        "            try:\n",
        "                price = int(float(s))\n",
        "            except Exception:\n",
        "                price = None\n",
        "\n",
        "    if not price or price <= 0:\n",
        "        return None\n",
        "\n",
        "    k = int(price // 1000)           # convert to thousands\n",
        "    k_floor = (k // floor_to_k) * floor_to_k\n",
        "    min_k = max(k_floor, 50)         # guardrail: at least $50k\n",
        "    max_k = min_k + spread_k\n",
        "    return (min_k, max_k)\n",
        "\n",
        "# --- Helper: prefer superset cache; fall back to partial (upper-bound) reuse ---\n",
        "\n",
        "def _get_superset_cached_df(\n",
        "    *,\n",
        "    mode: str,\n",
        "    zip_code: str,\n",
        "    min_price_k: int,\n",
        "    max_price_k: int,\n",
        "    max_pages_to_scrape: int,\n",
        "    cache_ttl_seconds: int | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Tries to reuse a *superset* cached scrape (kmin <= min_price_k and kmax >= max_price_k).\n",
        "    If not found, falls back to a *partial* reuse: any cached band that at least covers the\n",
        "    requested upper bound (kmax >= max_price_k). In that case, we filter to the overlap.\n",
        "\n",
        "    Returns a filtered DataFrame or None.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        cache = _listings_cache  # from scrape_redfin_to_df\n",
        "    except NameError:\n",
        "        return None\n",
        "\n",
        "    now = time.time()\n",
        "    best_superset = None\n",
        "    best_superset_ts = -1\n",
        "\n",
        "    best_partial = None\n",
        "    best_partial_ts = -1\n",
        "    best_partial_key = None\n",
        "    best_partial_kmin = None\n",
        "    best_partial_kmax = None\n",
        "\n",
        "    for key, (df, ts) in cache.items():\n",
        "        try:\n",
        "            m, z, kmin, kmax, pages = key\n",
        "        except Exception:\n",
        "            continue\n",
        "        if m != mode or z != zip_code or pages != max_pages_to_scrape:\n",
        "            continue\n",
        "        if cache_ttl_seconds is not None and (now - ts) >= cache_ttl_seconds:\n",
        "            continue\n",
        "\n",
        "        # SUPerset: covers full requested range\n",
        "        min_ok = (kmin is None) or (kmin <= min_price_k)\n",
        "        max_ok = (kmax is None) or (kmax >= max_price_k)\n",
        "        if min_ok and max_ok and ts > best_superset_ts:\n",
        "            best_superset, best_superset_ts = (key, df), ts\n",
        "            continue\n",
        "\n",
        "        # PARTIAL: at least covers the requested upper bound (<= X)\n",
        "        # e.g., cached 300–500k and user asks 100–450k; we can still reuse 300–450k slice\n",
        "        if (kmax is None or kmax >= max_price_k) and ts > best_partial_ts:\n",
        "            best_partial = df\n",
        "            best_partial_ts = ts\n",
        "            best_partial_key = key\n",
        "            best_partial_kmin = kmin\n",
        "            best_partial_kmax = kmax\n",
        "\n",
        "    # Prefer strict superset\n",
        "    if best_superset is not None:\n",
        "        key, df = best_superset\n",
        "        lo = min_price_k * 1000\n",
        "        hi = max_price_k * 1000\n",
        "        out = df.copy()\n",
        "        out[\"price\"] = pd.to_numeric(out[\"price\"], errors=\"coerce\")\n",
        "        out = out[(out[\"price\"] >= lo) & (out[\"price\"] <= hi)]\n",
        "        print(f\"[CACHE SUBSET HIT] superset {key} → subset {len(out)} rows\")\n",
        "        return out\n",
        "\n",
        "    # Otherwise, reuse partial coverage if available\n",
        "    if best_partial is not None:\n",
        "        df = best_partial.copy()\n",
        "        df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
        "        lo = min_price_k * 1000\n",
        "        hi = max_price_k * 1000\n",
        "\n",
        "        # Only confident coverage where cached band exists.\n",
        "        # If cached min (kmin) is above requested min, we still filter from that kmin.\n",
        "        if best_partial_kmin is not None:\n",
        "            lo = max(lo, best_partial_kmin * 1000)\n",
        "        if best_partial_kmax is not None:\n",
        "            hi = min(hi, best_partial_kmax * 1000)\n",
        "\n",
        "        out = df[(df[\"price\"] >= lo) & (df[\"price\"] <= hi)]\n",
        "        print(f\"[CACHE PARTIAL HIT] using {best_partial_key} → subset {len(out)} rows\")\n",
        "        return out\n",
        "\n",
        "    # No cache suitable\n",
        "    return None\n",
        "\n",
        "\n",
        "def _get_subject_geocode_cached(address: str, api_key: str, ttl_seconds: int | None = 864000):\n",
        "    \"\"\"Return (lat, lon, formatted, postcode) using a small cache.\"\"\"\n",
        "    key = (address or \"\").strip().lower()\n",
        "    now = time.time()\n",
        "    if key in _subject_geocode_cache:\n",
        "        lat, lon, fmt, pc, ts = _subject_geocode_cache[key]\n",
        "        if ttl_seconds is None or (now - ts) < ttl_seconds:\n",
        "            print(f\"[GEO CACHE HIT] subject '{address}'\")\n",
        "            return lat, lon, fmt, pc\n",
        "        else:\n",
        "            print(f\"[GEO CACHE EXPIRED] subject '{address}'\")\n",
        "\n",
        "    lat, lon, fmt, pc = _geoapify_geocode(address, api_key, pause=0.0)\n",
        "    if lat is not None and lon is not None:\n",
        "        _subject_geocode_cache[key] = (lat, lon, fmt, pc, now)\n",
        "        print(f\"[GEO CACHED] subject '{address}' -> ({lat:.6f},{lon:.6f})\")\n",
        "    return lat, lon, fmt, pc\n",
        "\n",
        "def _geocode_df_addresses_cached(\n",
        "    base_df: pd.DataFrame,\n",
        "    address_col: str,\n",
        "    zip_hint: str,\n",
        "    api_key: str,\n",
        "    cache_key: tuple,\n",
        "    ttl_seconds: int | None = 86400,\n",
        "    max_to_geocode: int = 250,\n",
        "    sleep_sec: float = 0.12,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Reuse a cached geocoded DF for comps; otherwise geocode once and cache.\"\"\"\n",
        "    now = time.time()\n",
        "    if cache_key in _geocoded_comps_cache:\n",
        "        cached_df, ts = _geocoded_comps_cache[cache_key]\n",
        "        if ttl_seconds is None or (now - ts) < ttl_seconds:\n",
        "            print(f\"[GEO CACHE HIT] {cache_key} -> {len(cached_df)} rows\")\n",
        "            return cached_df.copy()\n",
        "        else:\n",
        "            print(f\"[GEO CACHE EXPIRED] {cache_key}\")\n",
        "\n",
        "    geocoded = _geocode_df_addresses(\n",
        "        base_df, address_col, zip_hint, api_key,\n",
        "        max_to_geocode=max_to_geocode, sleep_sec=sleep_sec\n",
        "    )\n",
        "    _geocoded_comps_cache[cache_key] = (geocoded.copy(), now)\n",
        "    print(f\"[GEO CACHED] {cache_key} -> {len(geocoded)} rows\")\n",
        "    return geocoded\n"
      ],
      "metadata": {
        "id": "41gMlD9jGRY9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BxMXYGBVjepn"
      },
      "outputs": [],
      "source": [
        "# Cell 3.1: Define tool 1: TOOL FOR CALLING REAL ESTATE STATS\n",
        "from typing import Optional\n",
        "import requests\n",
        "import io\n",
        "\n",
        "@tool\n",
        "def get_zipcode_stats(zip_code: str, property_type: str = 'All Residential') -> str:\n",
        "    \"\"\"\n",
        "    Retrieves the latest real estate market statistics for a given US ZIP code and property type.\n",
        "    Use this when a user asks for general questions about a specific zipcode or city OR when the user asks for real estate statistics for a zipcode or city.\n",
        "\n",
        "    Args:\n",
        "        zip_code (str): The 5-digit US ZIP code to query.\n",
        "        property_type (str): The type of property. Must be one of:\n",
        "                             'Condo/Co-op', 'Townhouse', 'Single Family Residential',\n",
        "                             'All Residential', 'Multi-Family (2-4 Unit)'.\n",
        "                             Defaults to 'All Residential' if not specified.\n",
        "\n",
        "    Returns:\n",
        "        str: Returns all stats in the output followed by a summary of the latest market stats, or an error message if the\n",
        "             ZIP code or property type is not found.\n",
        "    \"\"\"\n",
        "    print(f\"🔧 TOOL get_zipcode_stats called with zip_code={zip_code}, property_type={property_type}\")\n",
        "    if data_subset is None:\n",
        "        return \"Error: The real estate dataset is not available.\"\n",
        "\n",
        "    # Pad the zip_code if a user enters something like \"9021\"\n",
        "    zip_code = zip_code.zfill(5)\n",
        "\n",
        "    # Filter data for the requested zip code and property type\n",
        "    filtered_data = data_subset[\n",
        "        (data_subset['zip_code'] == zip_code) &\n",
        "        (data_subset['property_type'] == property_type)\n",
        "    ]\n",
        "\n",
        "    if filtered_data.empty:\n",
        "        return f\"No data found for ZIP code {zip_code} with property type '{property_type}'.\"\n",
        "\n",
        "    # Get the most recent data\n",
        "    latest_data = filtered_data.sort_values(by='period_end', ascending=False).iloc[0]\n",
        "\n",
        "    # Format the output nicely\n",
        "    output = (\n",
        "          f\"The following are the stats for zip code '{zip_code}' as of '{latest_data['period_end'].strftime('%Y-%m-%d')}':\\n\"\n",
        "          f\"The Median Sale Price = '${latest_data['median_sale_price']:,.0f}'\\n\"\n",
        "          f\"The Month over Month change in sale Price (%) = '{latest_data['median_sale_price_mom'] * 100:.2f}'\\n\"\n",
        "          f\"The Year over Year change in sale Price (%) = '{latest_data['median_sale_price_yoy'] * 100:.2f}'\\n\"\n",
        "          f\"The Median List price = '${latest_data['median_list_price']:,.2f}'\\n\"\n",
        "          f\"The Month over Month change in list price (%) = '{latest_data['median_list_price_mom'] * 100:.2f}'\\n\"\n",
        "          f\"The Year over Year change in list price (%) = '{latest_data['median_list_price_yoy'] * 100:.2f}'\\n\"\n",
        "          f\"The Avg Sale to List Price (%) = '{latest_data['avg_sale_to_list'] * 100:.2f}'\\n\"\n",
        "          f\"Homes sold in the last 90 days = '{latest_data['homes_sold']:.0f}'\\n\"\n",
        "          f\"The Month over Month change in homes sold (%) = '{latest_data['homes_sold_mom'] * 100:.2f}'\\n\"\n",
        "          f\"The Year over Year change in homes sold (%) = '{latest_data['homes_sold_yoy'] * 100:.2f}'\\n\"\n",
        "          f\"Days on Market = '{latest_data['median_dom']:.0f}'\\n\"\n",
        "          f\"The Month over Month change in Days on Market = '{latest_data['median_dom_mom'] * 1:.0f}'\\n\"\n",
        "          f\"The Year over Year change in Days on Market = '{latest_data['median_dom_yoy'] * 1:.0f}'\\n\"\n",
        "    )\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8FLhQmqLmbzR"
      },
      "outputs": [],
      "source": [
        "# Cell 3.2: TOOL FOR ENDING THE CONVERSATION\n",
        "@tool\n",
        "def end_conversation() -> str:\n",
        "    \"\"\"\n",
        "    Call this specific tool when the user indicates they are finished with the conversation.\n",
        "    Use this for phrases like 'thank you', 'that's all', 'I'm done', 'thanks for the help', etc.\n",
        "    This tool signals that the conversation should end.\n",
        "    \"\"\"\n",
        "    return \"Conversation ended by user.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "x8dxmFLimnm_"
      },
      "outputs": [],
      "source": [
        "# Cell 3.3: TOOL FOR CALLING RECENTLY SOLD HOMES\n",
        "from typing import Optional, Union\n",
        "import requests, io\n",
        "import pandas as pd\n",
        "\n",
        "@tool\n",
        "def list_recently_sold_homes(\n",
        "    zip_code: str,\n",
        "    property_type: Optional[str] = None,\n",
        "    min_price: Optional[int] = None,\n",
        "    max_price: Optional[int] = None,\n",
        "    num_results: Union[int, str] = 5\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Fetches a DataFrame of homes sold in the last 90 days for a given ZIP code,\n",
        "    applies optional filters (property_type, min_price, max_price), and returns\n",
        "    a formatted string listing up to `num_results` homes.\n",
        "    If num_results <= 0, returns *all* matching homes (up to a practical limit).\n",
        "    Use this when the user asks for a list of recently sold homes or for any stats around homes sold.\n",
        "    \"\"\"\n",
        "    print(f\"🔧 TOOL list_recently_sold_homes called with zip_code={zip_code}, num_results={num_results}\")\n",
        "    # 1) Normalize num_results\n",
        "    try:\n",
        "        num_results = int(num_results)\n",
        "    except (TypeError, ValueError):\n",
        "        return \"⚠️ Error: `num_results` must be an integer.\"\n",
        "    # If negative or zero → show everything\n",
        "    show_all = num_results <= 0\n",
        "\n",
        "    # 2) Look up region identifiers from your preloaded redfin subset\n",
        "    if data_subset is None:\n",
        "        return \"Error: real-estate dataset unavailable.\"\n",
        "    df_meta = data_subset[data_subset[\"zip_code\"] == str(zip_code)]\n",
        "    if df_meta.empty:\n",
        "        return f\"No region data found for ZIP code {zip_code}.\"\n",
        "    region_id = str(df_meta[\"region_id\"].iloc[0])\n",
        "    region_type = int(df_meta[\"region_type\"].iloc[0])\n",
        "\n",
        "    # 3) Hit Redfin’s CSV API\n",
        "    params = {\n",
        "        \"region_id\": region_id,\n",
        "        \"region_type\": region_type,\n",
        "        \"sold_within_days\": 90,\n",
        "        \"al\": 1\n",
        "    }\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    try:\n",
        "        resp = requests.get(\"https://www.redfin.com/stingray/api/gis-csv\",\n",
        "                             params=params, headers=headers, timeout=10)\n",
        "        sold_df = pd.read_csv(io.BytesIO(resp.content))\n",
        "    except Exception as e:\n",
        "        return f\"Could not fetch sales data: {e}\"\n",
        "\n",
        "    # Debug: raw row count\n",
        "    print(f\"🔧 Retrieved {len(sold_df)} rows of data\")\n",
        "\n",
        "    # 4) Clean & filter\n",
        "    cols = ['PROPERTY TYPE','ADDRESS','PRICE','BEDS','BATHS','SQUARE FEET', 'DAYS ON MARKET']\n",
        "    sold_df = sold_df[cols].copy().fillna({\n",
        "        'PROPERTY TYPE': 'Unknown',\n",
        "        'ADDRESS': 'Unknown',\n",
        "        'PRICE': 0,\n",
        "        'BEDS': 0,\n",
        "        'BATHS': 0,\n",
        "        'SQUARE FEET': 0,\n",
        "        'DAYS ON MARKET': 0\n",
        "    })\n",
        "    # ensure numeric\n",
        "    for c in ['PRICE','BEDS','BATHS','SQUARE FEET', 'DAYS ON MARKET']:\n",
        "        sold_df[c] = sold_df[c].astype(int)\n",
        "    sold_df = sold_df[sold_df['PRICE'] > 1]\n",
        "\n",
        "    # 5) Apply user filters\n",
        "    if property_type:\n",
        "        # map user‐friendly names to API values\n",
        "        type_map = {\n",
        "            'single family residential': 'Single Family Residential',\n",
        "            'condo/co-op': 'Condo/Co-op',\n",
        "            'townhouse': 'Townhouse'\n",
        "        }\n",
        "        target = type_map.get(property_type.lower())\n",
        "        if target:\n",
        "            sold_df = sold_df[sold_df['PROPERTY TYPE'] == target]\n",
        "    if min_price is not None:\n",
        "        sold_df = sold_df[sold_df['PRICE'] >= min_price]\n",
        "    if max_price is not None:\n",
        "        sold_df = sold_df[sold_df['PRICE'] <= max_price]\n",
        "\n",
        "    # Debug: filtered row count\n",
        "    print(f\"🔧 Filtered to {len(sold_df)} rows of data\")\n",
        "\n",
        "    if sold_df.empty:\n",
        "        return \"No homes match your criteria. Try adjusting your filters.\"\n",
        "\n",
        "    # 6) Build the output\n",
        "    total_available = len(sold_df)\n",
        "    to_show = total_available if show_all else min(num_results, total_available)\n",
        "    header = (\n",
        "        f\"Here {'are all' if show_all else 'are'} \"\n",
        "        f\"{to_show} of {total_available} homes sold in {zip_code}:\\n\"\n",
        "    )\n",
        "\n",
        "    lines = []\n",
        "    for _, row in sold_df.head(to_show).iterrows():\n",
        "        lines.append(\n",
        "            f\"- {row['ADDRESS']} ({row['PROPERTY TYPE']})\\n\"\n",
        "            f\"  Price: ${row['PRICE']:,} | Beds: {row['BEDS']} | \"\n",
        "            f\"Baths: {row['BATHS']} | SqFt: {row['SQUARE FEET']}\\n\"\n",
        "        )\n",
        "\n",
        "    return header + \"\".join(lines)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.4: Tool for Finding Homes for Sale with cache (random sample ≤10, reuses superset cache, supports bed/bath/sqft filters)\n",
        "\n",
        "@tool\n",
        "def list_homes_for_sale(\n",
        "    zip_code: str,\n",
        "    max_price: int,\n",
        "    min_price: int = 100,\n",
        "    max_pages_to_scrape: int = 3,\n",
        "    min_beds: int | None = None,\n",
        "    min_baths: float | None = None,\n",
        "    min_sqft: int | None = None,              # <-- NEW\n",
        "    cache_ttl_seconds: int | None = 36000,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Scrapes Redfin.com for homes for sale within a ZIP & price range,\n",
        "    but first tries to reuse a cached *superset* scrape to avoid re-scraping.\n",
        "    Returns a random sample of up to 10 homes. Supports optional min_beds/min_baths/min_sqft filters.\n",
        "\n",
        "    Price inputs are in *thousands* (e.g., 300 → $300,000).\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"--- TOOL: `list_homes_for_sale` STARTED for ZIP {zip_code} (${min_price}k–${max_price}k) ---\")\n",
        "\n",
        "    # 1) Try to reuse a superset cache to avoid scraping again\n",
        "    final_df = _get_superset_cached_df(\n",
        "        mode=\"sale\",\n",
        "        zip_code=zip_code,\n",
        "        min_price_k=min_price,\n",
        "        max_price_k=max_price,\n",
        "        max_pages_to_scrape=max_pages_to_scrape,\n",
        "        cache_ttl_seconds=cache_ttl_seconds,\n",
        "    )\n",
        "\n",
        "    # 2) If no suitable superset cache, scrape (scrape_redfin_to_df has its own cache too)\n",
        "    if final_df is None:\n",
        "        final_df = scrape_redfin_to_df(\n",
        "            mode=\"sale\",\n",
        "            zip_code=zip_code,\n",
        "            min_price=min_price,\n",
        "            max_price=max_price,\n",
        "            max_pages_to_scrape=max_pages_to_scrape,\n",
        "            cache_ttl_seconds=cache_ttl_seconds,\n",
        "        )\n",
        "\n",
        "    if final_df is None or final_df.empty:\n",
        "        return (\n",
        "            f\"I was unable to find or extract any homes for sale in ZIP code {zip_code} \"\n",
        "            f\"between ${min_price}k and ${max_price}k.\"\n",
        "        )\n",
        "\n",
        "    # 3) De-dupe by address\n",
        "    df = final_df.copy()\n",
        "    if \"address\" in df.columns:\n",
        "        df = df.drop_duplicates(subset=[\"address\"])\n",
        "\n",
        "    # 4) Normalize numerics; support both \"sq ft\" and \"sqft\" column names\n",
        "    for col in [\"bedrooms\", \"bathrooms\", \"price\", \"sq ft\", \"sqft\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    # Ensure we have a unified \"sq ft\" column for filtering/printing\n",
        "    if \"sq ft\" not in df.columns and \"sqft\" in df.columns:\n",
        "        df[\"sq ft\"] = df[\"sqft\"]\n",
        "\n",
        "    # Apply optional bed/bath/sqft filters (treat as minimums)\n",
        "    if min_beds is not None and \"bedrooms\" in df.columns:\n",
        "        df = df[df[\"bedrooms\"] >= min_beds]\n",
        "\n",
        "    if min_baths is not None and \"bathrooms\" in df.columns:\n",
        "        df = df[df[\"bathrooms\"] >= float(min_baths)]\n",
        "\n",
        "    if min_sqft is not None and \"sq ft\" in df.columns:         # <-- NEW\n",
        "        df = df[df[\"sq ft\"] >= int(min_sqft)]                  # <-- NEW\n",
        "\n",
        "    matched = len(df)\n",
        "\n",
        "    if matched == 0:\n",
        "        msg = (\n",
        "            f\"I found {len(final_df)} homes for sale in {zip_code} between \"\n",
        "            f\"${min_price}k and ${max_price}k, but none match your filters\"\n",
        "        )\n",
        "        if min_beds is not None:  msg += f\" (≥{min_beds} beds)\"\n",
        "        if min_baths is not None: msg += f\" (≥{min_baths} baths)\"\n",
        "        if min_sqft is not None:  msg += f\" (≥{min_sqft} sq ft)\"\n",
        "        msg += \". Try adjusting the criteria.\"\n",
        "        print(f\"--- TOOL: `list_homes_for_sale` FINISHED ---\")\n",
        "        return msg\n",
        "\n",
        "    # 5) Random sample ≤ 10\n",
        "    sample_n = min(10, matched)\n",
        "    sample_df = df.sample(n=sample_n, random_state=None)\n",
        "\n",
        "    # 6) Sort sampled set by price for readability (optional)\n",
        "    if \"price\" in sample_df.columns:\n",
        "        sample_df = sample_df.sort_values(\"price\", kind=\"stable\")\n",
        "\n",
        "    # 7) Build output\n",
        "    filt_bits = []\n",
        "    if min_beds is not None:   filt_bits.append(f\"≥{min_beds} beds\")\n",
        "    if min_baths is not None:  filt_bits.append(f\"≥{min_baths} baths\")\n",
        "    if min_sqft is not None:   filt_bits.append(f\"≥{min_sqft} sq ft\")   # <-- NEW\n",
        "    filt_str = f\" (filtered: {', '.join(filt_bits)})\" if filt_bits else \"\"\n",
        "\n",
        "    header = (\n",
        "        f\"I found {matched} matching homes in {zip_code} between \"\n",
        "        f\"${min_price}k and ${max_price}k{filt_str}. Showing {sample_n} randomly selected:\\n\"\n",
        "    )\n",
        "\n",
        "    lines = [header]\n",
        "    for _, row in sample_df.iterrows():\n",
        "        address = str(row.get(\"address\", \"N/A\"))\n",
        "        price_val = row.get(\"price\")\n",
        "        beds_val = row.get(\"bedrooms\")\n",
        "        baths_val = row.get(\"bathrooms\")\n",
        "        sqft_val = row.get(\"sq ft\") if \"sq ft\" in row else row.get(\"sqft\")\n",
        "\n",
        "        price_str = f\"${int(price_val):,}\" if pd.notna(price_val) else \"N/A\"\n",
        "        beds_str  = f\"{int(beds_val)}\" if pd.notna(beds_val) else \"N/A\"\n",
        "        baths_str = \"N/A\"\n",
        "        if pd.notna(baths_val):\n",
        "            try:    baths_str = f\"{float(baths_val):.1f}\"\n",
        "            except: baths_str = str(baths_val)\n",
        "        sqft_str = \"N/A\"\n",
        "        if pd.notna(sqft_val):\n",
        "            try:    sqft_str = f\"{int(float(sqft_val))}\"\n",
        "            except: sqft_str = str(sqft_val)\n",
        "\n",
        "        lines.append(\n",
        "            f\"- Address: {address}\\n\"\n",
        "            f\"  Price: {price_str} | Beds: {beds_str} | Baths: {baths_str} | SqFt: {sqft_str}\\n\"\n",
        "        )\n",
        "\n",
        "    lines.append(\"\\n(Ask for “more” to see another random set, or refine beds/baths/price/sqft.)\")\n",
        "\n",
        "    print(f\"--- TOOL: `list_homes_for_sale` FINISHED ---\")\n",
        "    return \"\".join(lines)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DFtgCRh5qL_W"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.5: Tool for Finding Homes for Rent (random sample + filters + cache TTL)\n",
        "\n",
        "@tool\n",
        "def list_homes_for_rent(\n",
        "    zip_code: str,\n",
        "    max_pages_to_scrape: int = 3,\n",
        "    # --- optional filters ---\n",
        "    min_beds: int | None = None,\n",
        "    min_baths: float | None = None,\n",
        "    min_sqft: int | None = None,\n",
        "    min_rent: int | None = None,   # dollars/month\n",
        "    max_rent: int | None = None,   # dollars/month\n",
        "    # sampling\n",
        "    limit: int = 10,\n",
        "    seed: int | None = None,\n",
        "    # cache TTL (seconds) so follow-ups reuse scrape\n",
        "    cache_ttl_seconds: int = 36000,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Scrapes Redfin.com for rentals in a ZIP, then applies optional filters and returns\n",
        "    a random sample (default 10). Uses cache TTL to avoid re-scraping on follow-ups.\n",
        "\n",
        "    Filters (all optional):\n",
        "      - min_beds, min_baths, min_sqft\n",
        "      - min_rent, max_rent (in dollars/month)\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"--- TOOL: `list_homes_for_rent` STARTED for ZIP {zip_code} ---\")\n",
        "\n",
        "    final_df = scrape_redfin_to_df(\n",
        "        mode=\"rent\",\n",
        "        zip_code=zip_code,\n",
        "        max_pages_to_scrape=max_pages_to_scrape,\n",
        "        cache_ttl_seconds=cache_ttl_seconds,\n",
        "    )\n",
        "\n",
        "    if final_df is None:\n",
        "        return f\"I was unable to find or extract any homes for rent in ZIP code {zip_code}.\"\n",
        "\n",
        "    # Normalize + clean\n",
        "    df = final_df.copy()\n",
        "    for col in [\"rent\", \"sq ft\", \"bedrooms\", \"bathrooms\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    # Basic quality filters\n",
        "    df = df[\n",
        "        df[\"address\"].astype(str).str.len().ge(10)\n",
        "        & df[\"rent\"].gt(0)\n",
        "        & df[\"sq ft\"].gt(0)\n",
        "    ].copy()\n",
        "\n",
        "    if df.empty:\n",
        "        return f\"Rental data was scraped for ZIP {zip_code}, but could not be cleaned into a valid list.\"\n",
        "\n",
        "    # Apply user filters\n",
        "    filters_applied = []\n",
        "    if min_beds is not None:\n",
        "        df = df[df[\"bedrooms\"].ge(min_beds)]\n",
        "        filters_applied.append(f\"≥{min_beds} beds\")\n",
        "    if min_baths is not None:\n",
        "        df = df[df[\"bathrooms\"].ge(min_baths)]\n",
        "        filters_applied.append(f\"≥{min_baths} baths\")\n",
        "    if min_sqft is not None:\n",
        "        df = df[df[\"sq ft\"].ge(min_sqft)]\n",
        "        filters_applied.append(f\"≥{min_sqft} sq ft\")\n",
        "    if min_rent is not None:\n",
        "        df = df[df[\"rent\"].ge(min_rent)]\n",
        "        filters_applied.append(f\"≥${min_rent:,}/mo\")\n",
        "    if max_rent is not None:\n",
        "        df = df[df[\"rent\"].le(max_rent)]\n",
        "        filters_applied.append(f\"≤${max_rent:,}/mo\")\n",
        "\n",
        "    total_matches = len(df)\n",
        "    if total_matches == 0:\n",
        "        filters_note = f\" with filters ({', '.join(filters_applied)})\" if filters_applied else \"\"\n",
        "        return f\"I didn’t find any rentals in {zip_code}{filters_note}. Try loosening the filters.\"\n",
        "\n",
        "    # Random sample\n",
        "    n = min(limit, total_matches)\n",
        "    sample_kwargs = {\"n\": n, \"replace\": False}\n",
        "    if seed is not None:\n",
        "        sample_kwargs[\"random_state\"] = seed\n",
        "    sample_df = df.sample(**sample_kwargs)\n",
        "\n",
        "    # Format output\n",
        "    filters_note = f\" (filters: {', '.join(filters_applied)})\" if filters_applied else \"\"\n",
        "    header = f\"I found {total_matches} homes for rent in {zip_code}{filters_note}. Showing {n} randomly selected:\\n\"\n",
        "\n",
        "    lines = [header]\n",
        "    for _, row in sample_df.iterrows():\n",
        "        beds = int(row[\"bedrooms\"]) if pd.notna(row[\"bedrooms\"]) else \"—\"\n",
        "        baths = f\"{float(row['bathrooms']):.1f}\" if pd.notna(row[\"bathrooms\"]) else \"—\"\n",
        "        sqft = int(row[\"sq ft\"]) if pd.notna(row[\"sq ft\"]) else \"—\"\n",
        "        rent_val = int(row[\"rent\"]) if pd.notna(row[\"rent\"]) else 0\n",
        "        lines.append(\n",
        "            f\"- Address: {row['address']}\\n\"\n",
        "            f\"  Rent: ${rent_val:,.0f}/mo | Beds: {beds} | Baths: {baths} | SqFt: {sqft}\\n\"\n",
        "        )\n",
        "\n",
        "    lines.append(\"\\n(Ask for “more” to see another random set, or refine beds/baths/sqft/rent.)\")\n",
        "    print(f\"--- TOOL: `list_homes_for_rent` FINISHED ---\")\n",
        "    return \"\".join(lines)\n"
      ],
      "metadata": {
        "id": "vwVBjKxs0jG4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.6: Tool for running competitive market assessment (now includes RECENTLY SOLD comps)\n",
        "\n",
        "from typing import Optional\n",
        "import io, json, time, requests\n",
        "import pandas as pd\n",
        "from groq import Groq\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def competitive_market_assessment(\n",
        "    address: str,\n",
        "    subject_bedrooms: int,\n",
        "    subject_bathrooms: float,\n",
        "    subject_sqft: int,\n",
        "    radius_miles: float = 3.0,\n",
        "    max_comps: int = 15,\n",
        "    max_pages_to_scrape: int = 3,\n",
        "    sale_min_price_k: Optional[int] = None,\n",
        "    sale_max_price_k: Optional[int] = None,\n",
        "    cache_ttl_seconds: int = 36000,\n",
        "    refresh_scrape: bool = False,\n",
        "    groq_model: str = \"openai/gpt-oss-120b\"  # deepseek-r1-distill-llama-70b\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Geocode the subject; if sale band missing, infer it via Groq (compound-beta);\n",
        "    then fetch cached rentals, active for-sale listings, and RECENTLY SOLD comps.\n",
        "    Compute distance, pick nearest N; estimate rent & sale price (active + sold).\n",
        "    \"\"\"\n",
        "\n",
        "    # Keys\n",
        "    geo_key = geoapify_api_key\n",
        "    groq_key = groq_api_key\n",
        "\n",
        "    # 1) Geocode subject (cached)\n",
        "    lat, lon, formatted, postcode = _get_subject_geocode_cached(\n",
        "        address, geo_key, ttl_seconds=cache_ttl_seconds\n",
        "    )\n",
        "    if lat is None or lon is None:\n",
        "        return f\"Could not geocode the target address: {address}\"\n",
        "    zip_code = _zip_from_geocode(formatted, postcode)\n",
        "    if not zip_code:\n",
        "        return f\"Could not determine ZIP code for: {formatted or address}\"\n",
        "    print(f\"[CMA] Subject: {formatted or address} @ ({lat:.6f},{lon:.6f}) ZIP={zip_code}\")\n",
        "\n",
        "    # 1b) If sale band is missing, infer via LLM and continue\n",
        "    if (sale_min_price_k is None) or (sale_max_price_k is None):\n",
        "        print(\"[CMA] No sale band provided; probing approximate price via LLM…\")\n",
        "        inferred = _infer_sale_band_k_from_llm(formatted or address, groq_key, model=\"compound-beta\")\n",
        "        if inferred is None:\n",
        "            return (\n",
        "                f\"I couldn’t infer a sale price band for {formatted or address} (ZIP {zip_code}). \"\n",
        "                \"Please provide `sale_min_price_k` and `sale_max_price_k` in thousands, e.g., 300 and 500.\"\n",
        "            )\n",
        "        sale_min_price_k, sale_max_price_k = inferred\n",
        "        print(f\"[CMA] Inferred sale band: {sale_min_price_k}k–{sale_max_price_k}k\")\n",
        "\n",
        "    # Normalize band\n",
        "    if sale_min_price_k is not None and sale_min_price_k > 5000:\n",
        "        sale_min_price_k = int(round(sale_min_price_k / 1000))\n",
        "    if sale_max_price_k is not None and sale_max_price_k > 5000:\n",
        "        sale_max_price_k = int(round(sale_max_price_k / 1000))\n",
        "    if (sale_min_price_k is not None and sale_max_price_k is not None\n",
        "            and sale_min_price_k > sale_max_price_k):\n",
        "        sale_min_price_k, sale_max_price_k = sale_max_price_k, sale_min_price_k\n",
        "\n",
        "    # 2) Pull comps (uses your scrape cache for rent/active)\n",
        "    rent_df = scrape_redfin_to_df(\n",
        "        mode=\"rent\",\n",
        "        zip_code=zip_code,\n",
        "        max_pages_to_scrape=max_pages_to_scrape,\n",
        "        cache_ttl_seconds=cache_ttl_seconds\n",
        "    )\n",
        "    sale_df = scrape_redfin_to_df(\n",
        "        mode=\"sale\",\n",
        "        zip_code=zip_code,\n",
        "        max_pages_to_scrape=max_pages_to_scrape,\n",
        "        min_price=sale_min_price_k,\n",
        "        max_price=sale_max_price_k,\n",
        "        cache_ttl_seconds=cache_ttl_seconds\n",
        "    )\n",
        "\n",
        "    # NEW 2b) Pull RECENTLY SOLD comps via Redfin CSV (uses built-in lat/long; no geocode)\n",
        "    sold_df = None\n",
        "    try:\n",
        "        _ = data_subset  # ensure global exists\n",
        "        df_meta = data_subset[data_subset[\"zip_code\"] == str(zip_code)]\n",
        "        if not df_meta.empty:\n",
        "            region_id = str(df_meta[\"region_id\"].iloc[0])\n",
        "            region_type = int(df_meta[\"region_type\"].iloc[0])\n",
        "            params = {\n",
        "                \"region_id\": region_id,\n",
        "                \"region_type\": region_type,\n",
        "                \"sold_within_days\": 90,\n",
        "                \"al\": 1\n",
        "            }\n",
        "            headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "            resp = requests.get(\"https://www.redfin.com/stingray/api/gis-csv\",\n",
        "                                params=params, headers=headers, timeout=12)\n",
        "            raw = pd.read_csv(io.BytesIO(resp.content))\n",
        "            if not raw.empty:\n",
        "                cols = ['ADDRESS','PRICE','BEDS','BATHS','SQUARE FEET','LATITUDE','LONGITUDE','SOLD DATE']\n",
        "                present = [c for c in cols if c in raw.columns]\n",
        "                sold_df = raw[present].copy()\n",
        "                sold_df = sold_df.rename(columns={\n",
        "                    'ADDRESS':'address',\n",
        "                    'PRICE':'price',\n",
        "                    'BEDS':'bedrooms',\n",
        "                    'BATHS':'bathrooms',\n",
        "                    'SQUARE FEET':'sq ft',\n",
        "                    'LATITUDE':'latitude',\n",
        "                    'LONGITUDE':'longitude',\n",
        "                    'SOLD DATE':'sold_date'\n",
        "                })\n",
        "                # numeric normalize\n",
        "                for c in ['price','bedrooms','bathrooms','sq ft','latitude','longitude']:\n",
        "                    if c in sold_df.columns:\n",
        "                        sold_df[c] = pd.to_numeric(sold_df[c], errors='coerce')\n",
        "                # drop invalid coords/prices\n",
        "                sold_df = sold_df.dropna(subset=['latitude','longitude','price'])\n",
        "                if (sale_min_price_k is not None) or (sale_max_price_k is not None):\n",
        "                  lo = sale_min_price_k * 1000 if sale_min_price_k is not None else -float(\"inf\")\n",
        "                  hi = sale_max_price_k * 1000 if sale_max_price_k is not None else  float(\"inf\")\n",
        "                  pre_rows = len(sold_df)\n",
        "                  sold_df = sold_df[(sold_df[\"price\"] >= lo) & (sold_df[\"price\"] <= hi)]\n",
        "                  print(f\"[CMA] Sold comps price-band filter: {pre_rows} → {len(sold_df)} rows \"\n",
        "                  f\"(band {sale_min_price_k}k–{sale_max_price_k}k)\")\n",
        "        else:\n",
        "            print(f\"[CMA] No region meta for ZIP {zip_code}; skipping sold comps.\")\n",
        "    except NameError:\n",
        "        print(\"[CMA] data_subset not available; skipping sold comps.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[CMA] Sold comps fetch error: {e}\")\n",
        "\n",
        "    if (rent_df is None or rent_df.empty) and (sale_df is None or sale_df.empty) and (sold_df is None or sold_df.empty):\n",
        "        return f\"No comps available for ZIP {zip_code}. Try a different area or widen search.\"\n",
        "\n",
        "    # 3) Geocode/measure distance + pick nearest\n",
        "    radius_km = radius_miles * 1.60934\n",
        "    comps = {}\n",
        "\n",
        "    # RENT (geocode cached)\n",
        "    if rent_df is not None and not rent_df.empty:\n",
        "        rent_key = (\"rent\", zip_code, None, None, max_pages_to_scrape)\n",
        "        rent_geo = _geocode_df_addresses_cached(\n",
        "            rent_df, \"address\", zip_code, geo_key,\n",
        "            cache_key=rent_key, ttl_seconds=cache_ttl_seconds\n",
        "        )\n",
        "        rent_geo[\"distance_km\"] = rent_geo.apply(\n",
        "            lambda r: _haversine_km(lat, lon, r[\"latitude\"], r[\"longitude\"])\n",
        "            if pd.notna(r[\"latitude\"]) and pd.notna(r[\"longitude\"]) else float(\"inf\"),\n",
        "            axis=1\n",
        "        )\n",
        "        rent_geo = rent_geo[pd.to_numeric(rent_geo[\"distance_km\"], errors=\"coerce\") < float(\"inf\")]\n",
        "        rent_geo = rent_geo.sort_values(\"distance_km\")\n",
        "        within = rent_geo[rent_geo[\"distance_km\"] <= radius_km]\n",
        "        pool = within if not within.empty else rent_geo\n",
        "        comps[\"rent\"] = _select_nearest_by_distance_only(pool, max_comps, kind=\"rent\", min_distance_m=0).copy()\n",
        "        print(f\"[CMA] Rentals picked: {len(comps['rent'])}\")\n",
        "\n",
        "    # ACTIVE FOR-SALE (geocode cached)\n",
        "    if sale_df is not None and not sale_df.empty:\n",
        "        sale_key = (\"sale\", zip_code, sale_min_price_k, sale_max_price_k, max_pages_to_scrape)\n",
        "        sale_geo = _geocode_df_addresses_cached(\n",
        "            sale_df, \"address\", zip_code, geo_key,\n",
        "            cache_key=sale_key, ttl_seconds=cache_ttl_seconds\n",
        "        )\n",
        "        sale_geo[\"distance_km\"] = sale_geo.apply(\n",
        "            lambda r: _haversine_km(lat, lon, r[\"latitude\"], r[\"longitude\"])\n",
        "            if pd.notna(r[\"latitude\"]) and pd.notna(r[\"longitude\"]) else float(\"inf\"),\n",
        "            axis=1\n",
        "        )\n",
        "        sale_geo = sale_geo[pd.to_numeric(sale_geo[\"distance_km\"], errors=\"coerce\") < float(\"inf\")]\n",
        "        sale_geo = sale_geo.sort_values(\"distance_km\")\n",
        "        within = sale_geo[sale_geo[\"distance_km\"] <= radius_km]\n",
        "        pool = within if not within.empty else sale_geo\n",
        "        comps[\"sale\"] = _select_nearest_by_distance_only(pool, max_comps, kind=\"sale\", min_distance_m=0).copy()\n",
        "        print(f\"[CMA] For-sale picked: {len(comps['sale'])}\")\n",
        "\n",
        "    # NEW: RECENTLY SOLD (no geocode; distance from CSV lat/lon)\n",
        "    if sold_df is not None and not sold_df.empty:\n",
        "        sold_geo = sold_df.copy()\n",
        "        sold_geo[\"distance_km\"] = sold_geo.apply(\n",
        "            lambda r: _haversine_km(lat, lon, r[\"latitude\"], r[\"longitude\"])\n",
        "            if pd.notna(r[\"latitude\"]) and pd.notna(r[\"longitude\"]) else float(\"inf\"),\n",
        "            axis=1\n",
        "        )\n",
        "        sold_geo = sold_geo[pd.to_numeric(sold_geo[\"distance_km\"], errors=\"coerce\") < float(\"inf\")]\n",
        "        sold_geo = sold_geo.sort_values(\"distance_km\")\n",
        "        within = sold_geo[sold_geo[\"distance_km\"] <= radius_km]\n",
        "        pool = within if not within.empty else sold_geo\n",
        "        comps[\"sold\"] = pool.head(max_comps).copy()\n",
        "        print(f\"[CMA] Recently sold picked: {len(comps['sold'])}\")\n",
        "\n",
        "    # 4) Build JSON comps for prompts\n",
        "    def _records(df: pd.DataFrame, kind: str):\n",
        "        recs = []\n",
        "        for _, r in df.iterrows():\n",
        "            base = {\n",
        "                \"address\": str(r.get(\"address\",\"\")),\n",
        "                \"sq ft\": int(r.get(\"sq ft\", 0)) if pd.notna(r.get(\"sq ft\")) else 0,\n",
        "                \"bedrooms\": int(r.get(\"bedrooms\", 0)) if pd.notna(r.get(\"bedrooms\")) else 0,\n",
        "                \"bathrooms\": float(r.get(\"bathrooms\", 0)) if pd.notna(r.get(\"bathrooms\")) else 0.0,\n",
        "                \"latitude\": float(r.get(\"latitude\")) if pd.notna(r.get(\"latitude\")) else None,\n",
        "                \"longitude\": float(r.get(\"longitude\")) if pd.notna(r.get(\"longitude\")) else None,\n",
        "                \"distance_km\": float(r.get(\"distance_km\")) if pd.notna(r.get(\"distance_km\")) else None\n",
        "            }\n",
        "            if kind == \"rent\":\n",
        "                base[\"rent\"] = int(r.get(\"rent\", 0)) if pd.notna(r.get(\"rent\")) else 0\n",
        "            else:\n",
        "                base[\"price\"] = int(r.get(\"price\", 0)) if pd.notna(r.get(\"price\")) else 0\n",
        "            # If this is a sold comp and date exists, include it\n",
        "            if \"sold_date\" in r.index and pd.notna(r.get(\"sold_date\")):\n",
        "                base[\"sold_date\"] = str(r.get(\"sold_date\"))\n",
        "            recs.append(base)\n",
        "        return json.dumps(recs, indent=2)\n",
        "\n",
        "    rent_json = _records(comps[\"rent\"], \"rent\") if \"rent\" in comps else None\n",
        "    sale_json = _records(comps[\"sale\"], \"sale\") if \"sale\" in comps else None\n",
        "    sold_json = _records(comps[\"sold\"], \"sold\") if \"sold\" in comps else None  # NEW\n",
        "\n",
        "    # 5) Ask Groq to estimate\n",
        "    client = Groq(api_key=groq_key)\n",
        "\n",
        "    rent_section = \"\"\n",
        "    if rent_json and len(json.loads(rent_json)) > 0:\n",
        "        system_rent = (\n",
        "            \"You are an excellent real estate agent with a strong analytical mind. \"\n",
        "            \"Estimate monthly rent for the target using nearby comps. \"\n",
        "            \"Return a comprehensive rationale but dont share any inner chain-of-thought.\"\n",
        "        )\n",
        "        rent_msgs = [\n",
        "            {\"role\": \"system\", \"content\": system_rent},\n",
        "            {\"role\": \"user\", \"content\":\n",
        "                f\"Target: {formatted or address}\\n\"\n",
        "                f\"Beds: {subject_bedrooms}  Baths: {subject_bathrooms}  SqFt: {subject_sqft}\\n\\n\"\n",
        "                \"Nearby rental comps (JSON):\\n\"\n",
        "                f\"{rent_json}\\n\\n\"\n",
        "                \"Requirements:\\n\"\n",
        "                \"- Ignore any comp whose address exactly matches the target.\\n\"\n",
        "                \"- Prioritize similarity (beds/baths/sqft) over raw distance.\\n\"\n",
        "                \"- Provide an estimate with a narrow range (±5%).\\n\"\n",
        "                \"- List at least 4 comparable properties if available and explain why each was chosen.\\n\"\n",
        "                \"- Clearly articulate how these comparable properties influenced your estimate\"\n",
        "            }\n",
        "        ]\n",
        "        r = client.chat.completions.create(\n",
        "            model=groq_model, messages=rent_msgs, temperature=0.2, top_p=0.9, max_tokens=10000, reasoning_effort=\"medium\",\n",
        "        )\n",
        "        rent_section = (r.choices[0].message.content or \"\").strip()\n",
        "\n",
        "    sale_section = \"\"\n",
        "    if (sale_json and len(json.loads(sale_json)) > 0) or (sold_json and len(json.loads(sold_json)) > 0):\n",
        "        system_sale = (\n",
        "            \"You are an excellent real estate agent with a strong analytical mind. \"\n",
        "            \"Estimate the selling price for the target using BOTH nearby active listings and recently SOLD comps. \"\n",
        "            \"Analyze SOLD and ACTIVE separately, then reconcile. \"\n",
        "            \"Return a comprehensive rationale but do not reveal chain-of-thought.\"\n",
        "        )\n",
        "\n",
        "        # You can tweak these weights as you like\n",
        "        sold_weight = 0.3\n",
        "        active_weight = 0.7\n",
        "\n",
        "        sale_user = f\"\"\"\n",
        "    Target: {formatted or address}\n",
        "    Beds: {subject_bedrooms}  Baths: {subject_bathrooms}  SqFt: {subject_sqft}\n",
        "\n",
        "    ACTIVE_LISTINGS_JSON:\n",
        "    {sale_json or '[]'}\n",
        "\n",
        "    SOLD_COMPS_JSON:\n",
        "    {sold_json or '[]'}\n",
        "\n",
        "    Instructions (follow exactly):\n",
        "    1) Create two distinct sections. Do NOT mix rows:\n",
        "      - \"SOLD ANALYSIS\": choose the 3–6 strongest SOLD comps (closest + most similar beds/baths/sqft). For each, show address, beds, baths, sqft, price, $/sf, distance.\n",
        "        - Compute SOLD price-per-sf stats: median, low, high.\n",
        "        - Brief bullets on why each sold comp was chosen.\n",
        "      - \"ACTIVE ANALYSIS\": choose the 3–6 strongest ACTIVE listings. Same columns, separate table.\n",
        "        - Compute ACTIVE price-per-sf stats: median, low, high.\n",
        "        - Brief bullets on why each active comp was chosen.\n",
        "\n",
        "    2) Reconciliation (separate section titled \"RECONCILIATION & FINAL ESTIMATE\"):\n",
        "      - If BOTH sets exist: compute a blended $/sf = (SOLD_median * {sold_weight:.2f}) + (ACTIVE_median * {active_weight:.2f}).\n",
        "      - If only one set exists: use that set’s median $/sf (explain).\n",
        "      - Multiply blended $/sf by subject sqft to get a single point estimate.\n",
        "      - Provide a +/- 5% band around that number.\n",
        "      - 2–4 bullets explaining the key adjustments (beds/baths/sqft, recency, finish/condition signals).\n",
        "\n",
        "    3) Formatting rules:\n",
        "      - Two separate tables: one for SOLD, one for ACTIVE (omit a table if that set is empty and explicitly say so).\n",
        "      - Then a short \"RECONCILIATION & FINAL ESTIMATE\" section with the final dollar estimate and band.\n",
        "      - Keep it crisp; no chain-of-thought, just conclusions and brief justifications.\n",
        "    \"\"\"\n",
        "\n",
        "        sale_msgs = [\n",
        "            {\"role\": \"system\", \"content\": system_sale},\n",
        "            {\"role\": \"user\", \"content\": sale_user}\n",
        "        ]\n",
        "\n",
        "        s = client.chat.completions.create(\n",
        "            model=groq_model,\n",
        "            messages=sale_msgs,\n",
        "            temperature=0.2,\n",
        "            top_p=0.9,\n",
        "            max_tokens=10000,\n",
        "        )\n",
        "        sale_section = (s.choices[0].message.content or \"\").strip()\n",
        "\n",
        "\n",
        "    # 6) Final text\n",
        "    header = (\n",
        "        f\"🏠 Competitive Market Assessment\\n\"\n",
        "        f\"Subject: {formatted or address}\\n\"\n",
        "        f\"Beds: {subject_bedrooms} • Baths: {subject_bathrooms} • SqFt: {subject_sqft}\\n\"\n",
        "        f\"Radius: {radius_miles} mi • Max comps: {max_comps}\\n\"\n",
        "        f\"ZIP: {zip_code}\\n\"\n",
        "        f\"(Sale band used: {sale_min_price_k}k–{sale_max_price_k}k)\\n\"\n",
        "    )\n",
        "    parts = [header]\n",
        "    if rent_section:\n",
        "        parts.append(\"\\n=== Rent Estimate ===\\n\")\n",
        "        parts.append(rent_section)\n",
        "    if sale_section:\n",
        "        parts.append(\"\\n=== Sale Estimate (Actives + Sold) ===\\n\")\n",
        "        parts.append(sale_section)\n",
        "    if not rent_section and not sale_section:\n",
        "        parts.append(\"\\nNo comps could be evaluated.\")\n",
        "\n",
        "    # Save minimal snapshot for later cross-referencing\n",
        "    CMA_RESULTS.append({\n",
        "        \"address\": formatted or address,\n",
        "        \"zip_code\": zip_code,\n",
        "        \"beds\": subject_bedrooms,\n",
        "        \"baths\": subject_bathrooms,\n",
        "        \"sqft\": subject_sqft,\n",
        "        \"latitude\": lat,\n",
        "        \"longitude\": lon,\n",
        "        \"rent_section\": rent_section,\n",
        "        \"sale_section\": sale_section,\n",
        "        \"timestamp\": pd.Timestamp.utcnow().isoformat()\n",
        "    })\n",
        "    print(f\"[CMA] Saved CMA results for {formatted or address} (ZIP {zip_code})\")\n",
        "\n",
        "    return \"\".join(parts)\n",
        "\n",
        "print(\"✅ CMA increments loaded. (Actives + RECENTLY SOLD comps included.)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-TwVZCrp2C3",
        "outputId": "084f4434-c24b-4dab-a64a-4e275f6b2b54"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ CMA increments loaded. (Actives + RECENTLY SOLD comps included.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.7: Web Intelligence via compound-beta (simple pass-through)\n",
        "\n",
        "from typing import Optional, List, Tuple\n",
        "import time\n",
        "from textwrap import dedent\n",
        "from groq import Groq\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# ---- lightweight cache (keyed by query+addr+zip+domains+freshness) ----\n",
        "_webintel_cache: dict[tuple, tuple[str, float]] = {}\n",
        "\n",
        "def _norm(s: Optional[str]) -> Optional[str]:\n",
        "    return s.strip().lower() if isinstance(s, str) else None\n",
        "\n",
        "def _norm_domains(domains: Optional[List[str]]) -> Optional[Tuple[str, ...]]:\n",
        "    if not domains:\n",
        "        return None\n",
        "    return tuple(sorted({d.strip().lower() for d in domains if d and d.strip()}))\n",
        "\n",
        "@tool\n",
        "def web_intelligence_real_estate(\n",
        "    query: str,\n",
        "    address: Optional[str] = None,\n",
        "    zip_code: Optional[str] = None,\n",
        "    domains: Optional[List[str]] = None,\n",
        "    freshness_days: int = 7,\n",
        "    tokens_max: int = 1500,\n",
        "    cache_ttl_seconds: Optional[int] = 6 * 36000\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Simple pass-through real-estate search using Groq 'compound-beta'.\n",
        "    Returns EXACTLY the model's text content (no reformatting, no JSON coercion).\n",
        "\n",
        "    Usage:\n",
        "      raw_text = web_intelligence_real_estate.invoke({...})\n",
        "      # If you want reasoning/tools, print them yourself from the raw client response.\n",
        "    \"\"\"\n",
        "    # cache hit?\n",
        "    key = (_norm(query), _norm(address), _norm(zip_code), _norm_domains(domains), int(freshness_days))\n",
        "    now = time.time()\n",
        "    if cache_ttl_seconds is not None and key in _webintel_cache:\n",
        "        payload, ts = _webintel_cache[key]\n",
        "        if (now - ts) < cache_ttl_seconds:\n",
        "            return payload  # return exact cached text\n",
        "\n",
        "    # minimal system prompt (ask for concise, plain-text answer)\n",
        "    sys = dedent(f\"\"\"\n",
        "    You are a real-estate web intelligence assistant.\n",
        "    Answer in clear, plain English with concise facts only.\n",
        "    Prefer information published within the last {freshness_days} days when applicable.\n",
        "    Do NOT include extra sections, headers, or JSON. Just answer the user's question directly.\n",
        "    \"\"\").strip()\n",
        "\n",
        "    # optional guidance for the model (not shown to the user)\n",
        "    guidance_lines = []\n",
        "    if address:  guidance_lines.append(f\"Address focus: {address}\")\n",
        "    if zip_code: guidance_lines.append(f\"ZIP focus: {zip_code}\")\n",
        "    if domains:  guidance_lines.append(\"Prefer domains: \" + \", \".join(domains))\n",
        "    guidance = (\"\\n\" + \"\\n\".join(guidance_lines)) if guidance_lines else \"\"\n",
        "\n",
        "    user = f\"{query.strip()}{guidance}\"\n",
        "\n",
        "    try:\n",
        "        client = Groq(api_key=groq_api_key)\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"compound-beta\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": sys},\n",
        "                {\"role\": \"user\", \"content\": user}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=tokens_max,\n",
        "        )\n",
        "        content = (resp.choices[0].message.content or \"\").strip()\n",
        "\n",
        "        # cache & return exact text\n",
        "        _webintel_cache[key] = (content, now)\n",
        "        return content\n",
        "\n",
        "    except Exception as e:\n",
        "        # On any error, return a plain string (keeps the contract simple)\n",
        "        return f\"Search failed: {e}\"\n"
      ],
      "metadata": {
        "id": "GiGyGZcmVGbM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.8: NPV tool (hardened JSON parse + address canonicalization + inputs source echo)\n",
        "\n",
        "from typing import Optional, Dict, Any, Tuple, List\n",
        "import math, json, re, time\n",
        "import pandas as pd\n",
        "from groq import Groq\n",
        "from langchain_core.tools import tool\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# -------------------------------\n",
        "# Globals / caches\n",
        "# -------------------------------\n",
        "try:\n",
        "    CMA_RESULTS\n",
        "except NameError:\n",
        "    CMA_RESULTS = []\n",
        "\n",
        "try:\n",
        "    NPV_RESULTS\n",
        "except NameError:\n",
        "    NPV_RESULTS: List[Dict[str, Any]] = []\n",
        "\n",
        "# -------------------------------\n",
        "# Small helpers\n",
        "# -------------------------------\n",
        "def _extract_json_block(text: str) -> str:\n",
        "    \"\"\"Extract a JSON object from plain or ```json fenced``` text.\"\"\"\n",
        "    if not text:\n",
        "        return \"{}\"\n",
        "    t = text.strip()\n",
        "    m = re.search(r\"```(?:json)?\\s*(\\{.*?\\})\\s*```\", t, flags=re.S | re.I)\n",
        "    if m:\n",
        "        return m.group(1)\n",
        "    if t.startswith(\"{\") and t.endswith(\"}\"):\n",
        "        return t\n",
        "    i, j = t.find(\"{\"), t.rfind(\"}\")\n",
        "    if i != -1 and j > i:\n",
        "        return t[i:j+1]\n",
        "    return \"{}\"\n",
        "\n",
        "# -------------------------------\n",
        "# Address picking (best address match only)\n",
        "# -------------------------------\n",
        "def _norm_addr(s: str) -> str:\n",
        "    if not s: return \"\"\n",
        "    s = s.lower()\n",
        "    s = re.sub(r'[^a-z0-9 ]+', ' ', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    s = s.replace(\" street\", \" st\").replace(\" avenue\", \" ave\").replace(\" boulevard\", \" blvd\")\n",
        "    s = s.replace(\" drive\", \" dr\").replace(\" road\", \" rd\").replace(\" lane\", \" ln\").replace(\" trail\", \" trl\")\n",
        "    return s\n",
        "\n",
        "def _addr_similarity(a: str, b: str) -> float:\n",
        "    return SequenceMatcher(None, _norm_addr(a), _norm_addr(b)).ratio()\n",
        "\n",
        "def _has_sections(c):\n",
        "    return int(bool(c.get(\"rent_section\"))) + int(bool(c.get(\"sale_section\")))\n",
        "\n",
        "def _pick_cma_by_address(target_address: str, min_sim: float = 0.60):\n",
        "    try:\n",
        "        rows = list(CMA_RESULTS)\n",
        "    except NameError:\n",
        "        return None\n",
        "    if not rows:\n",
        "        return None\n",
        "    scored = []\n",
        "    for c in rows:\n",
        "        sim = _addr_similarity(target_address or \"\", c.get(\"address\", \"\"))\n",
        "        secs = _has_sections(c)\n",
        "        ts = 0.0\n",
        "        try: ts = pd.to_datetime(c.get(\"timestamp\")).timestamp()\n",
        "        except Exception: pass\n",
        "        scored.append((-sim, -secs, -ts, c, sim))\n",
        "    scored.sort()\n",
        "    best = scored[0][3]\n",
        "    best_sim = scored[0][4]\n",
        "    if best_sim < min_sim:\n",
        "        best = max(rows, key=lambda c: pd.to_datetime(c.get(\"timestamp\", \"1970-01-01\")), default=None)\n",
        "    return best\n",
        "\n",
        "# -------------------------------\n",
        "# LLM extractors (small/cheap model, NO regex fallback)\n",
        "# -------------------------------\n",
        "def _extract_sale_price_llm(sale_section_text: str, ctx: Dict[str, Any]) -> Optional[float]:\n",
        "    if not sale_section_text:\n",
        "        return None\n",
        "    sys = (\n",
        "        \"You extract ONE number as the best single-point estimate of market sale price \"\n",
        "        \"from the provided CMA narrative. Output ONLY JSON: {\\\"price_usd\\\": <number>} with no currency symbols.\"\n",
        "    )\n",
        "    hint = f\"Address: {ctx.get('address','')} | Beds: {ctx.get('beds')} | Baths: {ctx.get('baths')} | Sqft: {ctx.get('sqft')}\"\n",
        "    user = f\"{hint}\\n\\nCMA Sale Section:\\n{sale_section_text}\"\n",
        "    try:\n",
        "        client = Groq(api_key=groq_api_key)\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"openai/gpt-oss-20b\",\n",
        "            messages=[{\"role\":\"system\",\"content\":sys},{\"role\":\"user\",\"content\":user}],\n",
        "            temperature=0.0,\n",
        "            max_tokens=80,\n",
        "        )\n",
        "        txt = (resp.choices[0].message.content or \"\").strip()\n",
        "        data = json.loads(txt)\n",
        "        val = data.get(\"price_usd\")\n",
        "        return float(val) if val is not None else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _extract_rent_llm(rent_section_text: str, ctx: Dict[str, Any]) -> Optional[float]:\n",
        "    if not rent_section_text:\n",
        "        return None\n",
        "    sys = (\n",
        "        \"You extract ONE number as the best single-point estimate of MONTHLY RENT \"\n",
        "        \"from the provided CMA narrative. Output ONLY JSON: {\\\"rent_monthly_usd\\\": <number>} with no currency symbols.\"\n",
        "    )\n",
        "    hint = f\"Address: {ctx.get('address','')} | Beds: {ctx.get('beds')} | Baths: {ctx.get('baths')} | Sqft: {ctx.get('sqft')}\"\n",
        "    user = f\"{hint}\\n\\nCMA Rent Section:\\n{rent_section_text}\"\n",
        "    try:\n",
        "        client = Groq(api_key=groq_api_key)\n",
        "        resp = client.chat_completions.create(  # for some SDKs this is chat.completions; keep your original if needed\n",
        "            model=\"openai/gpt-oss-20b\",\n",
        "            messages=[{\"role\":\"system\",\"content\":sys},{\"role\":\"user\",\"content\":user}],\n",
        "            temperature=0.0,\n",
        "            max_tokens=80,\n",
        "        )\n",
        "        txt = (resp.choices[0].message.content or \"\").strip()\n",
        "        data = json.loads(txt)\n",
        "        val = data.get(\"rent_monthly_usd\")\n",
        "        return float(val) if val is not None else None\n",
        "    except Exception:\n",
        "        # If your SDK uses client.chat.completions, revert to that call signature.\n",
        "        try:\n",
        "            client = Groq(api_key=groq_api_key)\n",
        "            resp = client.chat.completions.create(\n",
        "                model=\"openai/gpt-oss-20b\",\n",
        "                messages=[{\"role\":\"system\",\"content\":sys},{\"role\":\"user\",\"content\":user}],\n",
        "                temperature=0.0,\n",
        "                max_tokens=80,\n",
        "            )\n",
        "            txt = (resp.choices[0].message.content or \"\").strip()\n",
        "            data = json.loads(txt)\n",
        "            val = data.get(\"rent_monthly_usd\")\n",
        "            return float(val) if val is not None else None\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "# -------------------------------\n",
        "# One-shot web intel (compound-beta) with robust JSON extraction\n",
        "# -------------------------------\n",
        "def _fetch_inputs_compound_beta(address: str, zip_code: Optional[str], state_hint: Optional[str], price_usd: float) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Single compound-beta call. Returns numeric inputs + a __meta flag telling you\n",
        "    if the data came from compound-beta or from defaults.\n",
        "    \"\"\"\n",
        "    sys = (\n",
        "        \"You are a real-estate investment inputs assistant. \"\n",
        "        \"Return ONLY strict JSON with numeric fields in decimals (not percents) and booleans. \"\n",
        "        \"Assume investor-occupied (non-owner) conventional 30-year fixed mortgage.\"\n",
        "    )\n",
        "    user = f\"\"\"\n",
        "Address: {address}\n",
        "ZIP: {zip_code or \"\"}\n",
        "State: {state_hint or \"\"}\n",
        "\n",
        "Target purchase price (USD): {price_usd:.2f}\n",
        "\n",
        "Return JSON with keys exactly:\n",
        "- mortgage_rate_30y: decimal (e.g., 0.063 for 6.3%), for investment property in the given state.\n",
        "- property_tax_rate: decimal of value (e.g., 0.018 = 1.8%).\n",
        "- insurance_annual_usd: number, estimated homeowner's insurance cost per year for a home near this price in this area.\n",
        "- hoa_monthly_usd: number (0 if none known).\n",
        "- inflation_rate: decimal (national CPI y/y if unsure).\n",
        "- appreciation_rate: decimal local home price appreciation trend (city/county best effort).\n",
        "- condition_ok: boolean; true if public listings suggest typical good condition; else false.\n",
        "\n",
        "Output ONLY compact JSON, no explanations.\n",
        "\"\"\".strip()\n",
        "\n",
        "    try:\n",
        "        client = Groq(api_key=groq_api_key)\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"compound-beta\",\n",
        "            messages=[{\"role\":\"system\",\"content\":sys},{\"role\":\"user\",\"content\":user}],\n",
        "            temperature=0.0,\n",
        "            max_tokens=300,\n",
        "        )\n",
        "        raw = (resp.choices[0].message.content or \"\").strip()\n",
        "        txt = _extract_json_block(raw)\n",
        "        data = json.loads(txt)\n",
        "\n",
        "        out = {\n",
        "            \"mortgage_rate_30y\": float(data.get(\"mortgage_rate_30y\", 0.065)),\n",
        "            \"property_tax_rate\": float(data.get(\"property_tax_rate\", 0.018)),\n",
        "            \"insurance_annual_usd\": float(data.get(\"insurance_annual_usd\", 1500.0)),\n",
        "            \"hoa_monthly_usd\": float(data.get(\"hoa_monthly_usd\", 0.0)),\n",
        "            \"inflation_rate\": float(data.get(\"inflation_rate\", 0.03)),\n",
        "            \"appreciation_rate\": float(data.get(\"appreciation_rate\", 0.02)),\n",
        "            \"condition_ok\": bool(data.get(\"condition_ok\", True)),\n",
        "            \"__meta\": {\"ok\": True, \"source\": \"compound-beta\", \"raw_len\": len(raw)}\n",
        "        }\n",
        "        return out\n",
        "    except Exception as e:\n",
        "        out = {\n",
        "            \"mortgage_rate_30y\": 0.065,\n",
        "            \"property_tax_rate\": 0.018,\n",
        "            \"insurance_annual_usd\": 1500.0,\n",
        "            \"hoa_monthly_usd\": 0.0,\n",
        "            \"inflation_rate\": 0.03,\n",
        "            \"appreciation_rate\": 0.02,\n",
        "            \"condition_ok\": True,\n",
        "            \"__meta\": {\"ok\": False, \"source\": \"defaults\", \"error\": str(e)}\n",
        "        }\n",
        "        return out\n",
        "\n",
        "# -------------------------------\n",
        "# Finance helpers\n",
        "# -------------------------------\n",
        "def _pmt(principal: float, annual_rate: float, years: int) -> float:\n",
        "    r = annual_rate / 12.0\n",
        "    n = years * 12\n",
        "    if r == 0:\n",
        "        return principal / n\n",
        "    return principal * (r * (1 + r)**n) / ((1 + r)**n - 1)\n",
        "\n",
        "def _balance_after(principal: float, annual_rate: float, years: int, months_paid: int) -> float:\n",
        "    r = annual_rate / 12.0\n",
        "    N = years * 12\n",
        "    n = months_paid\n",
        "    if r == 0:\n",
        "        return max(principal * (1 - n / N), 0)\n",
        "    return principal * (((1 + r)**N - (1 + r)**n) / ((1 + r)**N - 1))\n",
        "\n",
        "def _npv(discount_rate_annual: float, cashflows_by_year: List[float]) -> float:\n",
        "    r = discount_rate_annual\n",
        "    total = 0.0\n",
        "    for t, cf in enumerate(cashflows_by_year, start=1):\n",
        "        total += cf / ((1 + r) ** t)\n",
        "    return total\n",
        "\n",
        "# -------------------------------\n",
        "# NPV Tool\n",
        "# -------------------------------\n",
        "@tool\n",
        "def investment_npv(\n",
        "    address: str,\n",
        "    analysis_years: int = 30,\n",
        "    # optional overrides for what-if\n",
        "    override_price_usd: Optional[float] = None,\n",
        "    override_rent_monthly_usd: Optional[float] = None,\n",
        "    override_mortgage_rate_30y: Optional[float] = None,   # decimal\n",
        "    override_property_tax_rate: Optional[float] = None,   # decimal of value\n",
        "    override_insurance_annual_usd: Optional[float] = None,\n",
        "    override_hoa_monthly_usd: Optional[float] = None,\n",
        "    override_inflation_rate: Optional[float] = None,      # decimal\n",
        "    override_appreciation_rate: Optional[float] = None,\n",
        "    override_discount_rate_annual: Optional[float] = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Compute 30-year NPV and Year 0/1/5/10 cashflow + net return (incl. Y0 & cumulative CF).\n",
        "    Pull purchase price & rent from the most relevant CMA (address match), then fetch market inputs\n",
        "    via a single compound-beta call. Assumptions:\n",
        "      - Down payment: 25%\n",
        "      - Closing costs (Y0): 2.5% of price\n",
        "      - Selling costs: 7% of market value at sale\n",
        "      - Vacancy: 8.33% (1 month/yr)\n",
        "      - Mgmt: 8% EGI; Maint: 10% EGI; Misc: 5% EGI\n",
        "      - Taxes: Year-1 = rate × price; then grow by min(CPI, 0.5 × effective appreciation)\n",
        "      - Insurance (Y1): max(web, 0.5% × price); then grows with CPI\n",
        "      - HOA: grows at 0.5 × CPI\n",
        "      - Rent: grows with CPI\n",
        "      - Discount rate: safe rate by default (3%) unless overridden\n",
        "      - Improvements (Y0): 0 if condition_ok else 2% of price\n",
        "      - If (appreciation − inflation) > 2%, after Year 5 set appreciation = (CPI + 1%)\n",
        "    \"\"\"\n",
        "\n",
        "    # --- 1) Pick CMA entry by address (allows override-only mode) ---\n",
        "    cma = _pick_cma_by_address(address)\n",
        "\n",
        "    # Minimal ctx usable even without CMA\n",
        "    cma_ctx = {\n",
        "        \"address\": address if not cma else cma.get(\"address\",\"\"),\n",
        "        \"zip_code\": None if not cma else cma.get(\"zip_code\"),\n",
        "        \"beds\": None if not cma else cma.get(\"beds\"),\n",
        "        \"baths\": None if not cma else cma.get(\"baths\"),\n",
        "        \"sqft\": None if not cma else cma.get(\"sqft\"),\n",
        "    }\n",
        "    sale_text = \"\" if not cma else (cma.get(\"sale_section\",\"\") or \"\")\n",
        "    rent_text = \"\" if not cma else (cma.get(\"rent_section\",\"\") or \"\")\n",
        "\n",
        "    # --- 1a) Canonicalize address via geocoding (preferred over LLM \"correction\") ---\n",
        "    geo_key = globals().get(\"geoapify_api_key\")\n",
        "    canonical_addr, canonical_zip, state_hint = None, None, None\n",
        "    if geo_key:\n",
        "        try:\n",
        "            lat0, lon0, formatted0, postcode0 = _get_subject_geocode_cached(address, geo_key, ttl_seconds=3600)\n",
        "            if formatted0:\n",
        "                canonical_addr = formatted0\n",
        "            if postcode0:\n",
        "                mzip0 = re.search(r'\\b(\\d{5})(?:-\\d{4})?\\b', str(postcode0))\n",
        "                if mzip0:\n",
        "                    canonical_zip = mzip0.group(1)\n",
        "            mstate0 = re.search(r',\\s*([A-Z]{2})\\b', formatted0 or \"\")\n",
        "            if mstate0:\n",
        "                state_hint = mstate0.group(1)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Preferred address/ZIP for lookups\n",
        "    addr_for_parsing = canonical_addr or (cma_ctx.get(\"address\") or address or \"\")\n",
        "    if canonical_zip:\n",
        "        cma_ctx[\"zip_code\"] = canonical_zip\n",
        "    if not state_hint:\n",
        "        m = re.search(r',\\s*([A-Z]{2})\\b', addr_for_parsing)\n",
        "        if m:\n",
        "            state_hint = m.group(1)\n",
        "\n",
        "    # --- 2) Extract purchase price & monthly rent (allow overrides + partials) ---\n",
        "    price_usd = None\n",
        "    rent_monthly = None\n",
        "    if override_price_usd is not None:\n",
        "        price_usd = float(override_price_usd)\n",
        "    if override_rent_monthly_usd is not None:\n",
        "        rent_monthly = float(override_rent_monthly_usd)\n",
        "    if price_usd is None and sale_text:\n",
        "        price_usd = _extract_sale_price_llm(sale_text, cma_ctx)\n",
        "    if rent_monthly is None and rent_text:\n",
        "        rent_monthly = _extract_rent_llm(rent_text, cma_ctx)\n",
        "\n",
        "    if price_usd is None or rent_monthly is None:\n",
        "        if cma is None:\n",
        "            return (\"To run NPV without a CMA, please provide BOTH \"\n",
        "                    \"`override_price_usd` and `override_rent_monthly_usd`.\")\n",
        "        missing = []\n",
        "        if price_usd is None: missing.append(\"price\")\n",
        "        if rent_monthly is None: missing.append(\"rent\")\n",
        "        return (f\"I couldn’t extract {', '.join(missing)} from the CMA. \"\n",
        "                f\"Please pass override(s) for the missing value(s).\")\n",
        "\n",
        "    # --- 3) Market inputs (compound-beta) + overrides ---\n",
        "    wi = _fetch_inputs_compound_beta(addr_for_parsing, cma_ctx.get(\"zip_code\"), state_hint, price_usd)\n",
        "    mortgage_rate = override_mortgage_rate_30y if override_mortgage_rate_30y is not None else wi[\"mortgage_rate_30y\"]\n",
        "    tax_rate      = override_property_tax_rate   if override_property_tax_rate   is not None else wi[\"property_tax_rate\"]\n",
        "    ins_annual    = override_insurance_annual_usd if override_insurance_annual_usd is not None else wi[\"insurance_annual_usd\"]\n",
        "    hoa_monthly   = override_hoa_monthly_usd    if override_hoa_monthly_usd    is not None else wi[\"hoa_monthly_usd\"]\n",
        "    inflation     = override_inflation_rate     if override_inflation_rate     is not None else wi[\"inflation_rate\"]\n",
        "    appreciation  = override_appreciation_rate  if override_appreciation_rate  is not None else wi[\"appreciation_rate\"]\n",
        "    condition_ok  = wi[\"condition_ok\"]\n",
        "    web_meta      = wi.get(\"__meta\", {\"ok\": False, \"source\": \"unknown\"})\n",
        "\n",
        "    # Discount rate: safe rate default (3%) unless overridden\n",
        "    discount_rate = 0.03 if override_discount_rate_annual is None else float(override_discount_rate_annual)\n",
        "\n",
        "    # ---- (4) Assumptions & growth rules ----\n",
        "    down_pct = 0.25\n",
        "    closing_costs_pct = 0.025\n",
        "    selling_costs_pct = 0.07\n",
        "    vacancy_rate = 1.0/12.0\n",
        "    mgmt_pct = 0.08\n",
        "    maint_pct = 0.10\n",
        "    misc_pct = 0.05\n",
        "\n",
        "    # Insurance min rule (Year 1 baseline)\n",
        "    ins_annual = max(ins_annual, 0.005 * price_usd)\n",
        "    insurance_growth = inflation\n",
        "\n",
        "    # HOA growth\n",
        "    hoa_growth = 0.5 * inflation\n",
        "\n",
        "    # Temper appreciation after Year 5 if spread > 2%\n",
        "    use_tempered_after5 = (appreciation - inflation) > 0.02\n",
        "    late_appreciation = (inflation + 0.01) if use_tempered_after5 else appreciation\n",
        "\n",
        "    # Taxes growth: per-year min(CPI, 0.5 × effective_appreciation_t)\n",
        "    tax_base_year1 = tax_rate * price_usd\n",
        "\n",
        "    # Improvements at Y0\n",
        "    improve_cost = 0.0 if condition_ok else 0.02 * price_usd\n",
        "\n",
        "    # ---- (5) Loan setup ----\n",
        "    down_payment = down_pct * price_usd\n",
        "    closing_costs_y0 = closing_costs_pct * price_usd\n",
        "    loan_principal = price_usd - down_payment\n",
        "    monthly_pmt = _pmt(loan_principal, mortgage_rate, 30)\n",
        "    annual_debt_service = monthly_pmt * 12\n",
        "\n",
        "    # ---- (6) Year 0 cashflow (initial outlay) ----\n",
        "    y0_cashflow = -(down_payment + closing_costs_y0 + improve_cost)\n",
        "\n",
        "    # ---- (7) Annual projections ----\n",
        "    years = int(analysis_years)\n",
        "    cf_by_year, mv_by_year, bal_by_year = [], [], []\n",
        "    egi_by_year, noi_by_year = [], []\n",
        "    equity_by_year, cum_cf_by_year, net_return_incl_y0_by_year = [], [], []\n",
        "\n",
        "    rent_annual = rent_monthly * 12.0\n",
        "    hoa_annual = hoa_monthly * 12.0\n",
        "    insurance = ins_annual\n",
        "    value = price_usd\n",
        "    taxes_prev = tax_base_year1\n",
        "    cumulative_cf = 0.0\n",
        "\n",
        "    for t in range(1, years+1):\n",
        "        app_t = appreciation if t <= 5 else late_appreciation\n",
        "        value = value * (1 + app_t)\n",
        "        mv_by_year.append(value)\n",
        "\n",
        "        egi = rent_annual * (1 - vacancy_rate)\n",
        "        egi_by_year.append(egi)\n",
        "\n",
        "        tax_growth_t = min(inflation, 0.5 * app_t)\n",
        "        if t == 1:\n",
        "            taxes = taxes_prev\n",
        "        else:\n",
        "            taxes = taxes_prev * (1 + tax_growth_t)\n",
        "        taxes_prev = taxes\n",
        "\n",
        "        if t > 1:\n",
        "            insurance = insurance * (1 + insurance_growth)\n",
        "        hoa = hoa_annual * ((1 + hoa_growth) ** (t-1))\n",
        "\n",
        "        mgmt = mgmt_pct * egi\n",
        "        maint = maint_pct * egi\n",
        "        misc = misc_pct * egi\n",
        "\n",
        "        op_ex = taxes + insurance + hoa + mgmt + maint + misc\n",
        "        noi = egi - op_ex\n",
        "        noi_by_year.append(noi)\n",
        "\n",
        "        cf = noi - annual_debt_service\n",
        "        cf_by_year.append(cf)\n",
        "\n",
        "        rent_annual = rent_annual * (1 + inflation)\n",
        "\n",
        "        bal = _balance_after(loan_principal, mortgage_rate, 30, t*12)\n",
        "        bal_by_year.append(bal)\n",
        "        equity = value - bal\n",
        "        equity_by_year.append(equity)\n",
        "\n",
        "        cumulative_cf += cf\n",
        "        cum_cf_by_year.append(cumulative_cf)\n",
        "        net_return_incl_y0_by_year.append(equity + cumulative_cf + y0_cashflow)\n",
        "\n",
        "    # ---- (8) Terminal proceeds if sold end of final year ----\n",
        "    terminal_mv = mv_by_year[-1]\n",
        "    terminal_sell_costs = selling_costs_pct * terminal_mv\n",
        "    terminal_balance = bal_by_year[-1]\n",
        "    terminal_net_proceeds = terminal_mv - terminal_sell_costs - terminal_balance\n",
        "\n",
        "    # ---- (9) NPV (discount @ chosen safe rate by default) ----\n",
        "    npv_stream = cf_by_year[:-1] + [cf_by_year[-1] + terminal_net_proceeds]\n",
        "    npv_30y = y0_cashflow + _npv(discount_rate, npv_stream)\n",
        "\n",
        "    # ---- (10) Point-in-time summaries ----\n",
        "    def _pt(year_idx: int) -> Dict[str, float]:\n",
        "        i = year_idx - 1\n",
        "        if i < 0 or i >= years:\n",
        "            return {\"cashflow\": None, \"equity\": None, \"paper_total\": None,\n",
        "                    \"net_return_incl_y0\": None, \"net_proceeds_if_sold\": None}\n",
        "        cashflow = cf_by_year[i]\n",
        "        equity = equity_by_year[i]\n",
        "        paper_total = cashflow + equity\n",
        "        net_ret_y0 = net_return_incl_y0_by_year[i]\n",
        "        net_proceeds = mv_by_year[i] * (1 - selling_costs_pct) - bal_by_year[i]\n",
        "        return {\n",
        "            \"cashflow\": cashflow,\n",
        "            \"equity\": equity,\n",
        "            \"paper_total\": paper_total,\n",
        "            \"net_return_incl_y0\": net_ret_y0,\n",
        "            \"net_proceeds_if_sold\": net_proceeds\n",
        "        }\n",
        "\n",
        "    y1  = _pt(1)\n",
        "    y5  = _pt(5)\n",
        "    y10 = _pt(10)\n",
        "\n",
        "    # ---- (11) Persist full snapshot for what-ifs ----\n",
        "    snapshot = {\n",
        "        \"address\": addr_for_parsing,  # use canonicalized address if available\n",
        "        \"zip_code\": cma_ctx.get(\"zip_code\"),\n",
        "        \"beds\": cma_ctx.get(\"beds\"),\n",
        "        \"baths\": cma_ctx.get(\"baths\"),\n",
        "        \"sqft\": cma_ctx.get(\"sqft\"),\n",
        "        \"inputs\": {\n",
        "            \"price_usd\": price_usd,\n",
        "            \"rent_monthly_usd\": rent_monthly,\n",
        "            \"mortgage_rate_30y\": mortgage_rate,\n",
        "            \"property_tax_rate\": tax_rate,\n",
        "            \"insurance_annual_usd\": ins_annual,   # saved baseline (pre-growth)\n",
        "            \"hoa_monthly_usd\": hoa_monthly,\n",
        "            \"inflation_rate\": inflation,\n",
        "            \"appreciation_rate\": appreciation,\n",
        "            \"down_payment_pct\": 0.25,\n",
        "            \"closing_costs_pct_y0\": 0.025,\n",
        "            \"selling_costs_pct\": 0.07,\n",
        "            \"vacancy_rate\": 1.0/12.0,\n",
        "            \"mgmt_pct_of_egi\": 0.08,\n",
        "            \"maint_pct_of_egi\": 0.10,\n",
        "            \"misc_pct_of_egi\": 0.05,\n",
        "            \"hoa_growth\": 0.5 * inflation,\n",
        "            \"insurance_growth\": insurance_growth,\n",
        "            \"condition_ok\": condition_ok,\n",
        "            \"improvement_cost_y0\": improve_cost,\n",
        "            \"tempered_after_year5\": (appreciation - inflation) > 0.02,\n",
        "            \"late_appreciation_used\": (0.01 + inflation) if (appreciation - inflation) > 0.02 else appreciation,\n",
        "            \"discount_rate_annual\": discount_rate,\n",
        "            \"web_meta\": web_meta\n",
        "        },\n",
        "        \"derived\": {\n",
        "            \"down_payment_usd\": down_payment,\n",
        "            \"closing_costs_y0_usd\": closing_costs_y0,\n",
        "            \"loan_principal_usd\": loan_principal,\n",
        "            \"monthly_pmt_usd\": monthly_pmt,\n",
        "            \"annual_debt_service_usd\": annual_debt_service,\n",
        "            \"y0_cashflow_usd\": y0_cashflow,\n",
        "            \"npv_30y_usd\": npv_30y\n",
        "        },\n",
        "        \"series\": {\n",
        "            \"year\": list(range(1, years+1)),\n",
        "            \"egi\": egi_by_year,\n",
        "            \"noi\": noi_by_year,\n",
        "            \"cashflow\": cf_by_year,\n",
        "            \"cum_cashflow\": cum_cf_by_year,\n",
        "            \"market_value\": mv_by_year,\n",
        "            \"loan_balance\": bal_by_year,\n",
        "            \"equity\": equity_by_year,\n",
        "            \"net_return_incl_y0\": net_return_incl_y0_by_year\n",
        "        },\n",
        "        \"summaries\": {\n",
        "            \"year1\": y1,\n",
        "            \"year5\": y5,\n",
        "            \"year10\": y10,\n",
        "            \"terminal_net_proceeds_yearN\": terminal_net_proceeds\n",
        "        },\n",
        "        \"timestamp\": pd.Timestamp.utcnow().isoformat()\n",
        "    }\n",
        "    NPV_RESULTS.append(snapshot)\n",
        "    print(f\"[NPV] Saved NPV snapshot for {addr_for_parsing}\")\n",
        "\n",
        "    # ---- (12) Human-readable summary ----\n",
        "    def _fmt(n):\n",
        "        return f\"${n:,.0f}\" if n is not None else \"n/a\"\n",
        "    pct = lambda x: f\"{x*100:.2f}%\"\n",
        "\n",
        "    # Note if we standardized the address\n",
        "    addr_line = addr_for_parsing\n",
        "    if _norm_addr(addr_line) != _norm_addr(address):\n",
        "        addr_line += \"  _(standardized from your input)_\"\n",
        "\n",
        "    # Surface where inputs came from\n",
        "    source_line = f\"Inputs source: {web_meta.get('source','unknown')}\"\n",
        "    if not web_meta.get(\"ok\", False):\n",
        "        source_line += \" (fell back to defaults)\"\n",
        "\n",
        "    assumptions = (\n",
        "        f\"- Down payment: 25%  •  Closing costs (Y0): 2.5%  •  Selling costs: 7%\\n\"\n",
        "        f\"- Vacancy: 8.33%  •  Mgmt: 8% EGI  •  Maint: 10% EGI  •  Misc: 5% EGI\\n\"\n",
        "        f\"- Taxes: Year-1 = rate × price; thereafter grow by min(CPI, ½×effective appreciation)\\n\"\n",
        "        f\"- Insurance (Y1) = max(web, 0.5%×price); Insurance ↑ with CPI; HOA ↑ at 0.5×CPI; Rent ↑ with CPI\\n\"\n",
        "        f\"- If (appreciation − inflation) > 2%: after Year 5, appreciation set to (CPI + 1%)\\n\"\n",
        "        f\"- Discount rate = {pct(discount_rate)} (safe rate); \"\n",
        "        f\"Improvements at Y0: {_fmt(improve_cost)} ({'0%' if improve_cost == 0 else '2% of price'})\"\n",
        "    )\n",
        "\n",
        "    summary = [\n",
        "        f\"📍 **NPV Analysis for** {addr_line}\",\n",
        "        f\"{source_line}\",\n",
        "        f\"• Price: {_fmt(price_usd)}  • Rent (mo): {_fmt(rent_monthly)}\",\n",
        "        f\"• Mortgage rate (30y): {pct(mortgage_rate)}  • Inflation: {pct(inflation)}  • Appreciation: {pct(appreciation)}\",\n",
        "        f\"• Tax rate on value: {pct(tax_rate)}  • Insurance (yr1): {_fmt(max(ins_annual, 0.005*price_usd))}  • HOA (mo, yr1): {_fmt(hoa_monthly)}\",\n",
        "        \"\",\n",
        "        f\"**Year 0 cash outlay** = Downpayment + Closing + Improvements = {_fmt(-y0_cashflow)}\",\n",
        "        \"\",\n",
        "        f\"**Cashflow (Income − Expenses incl. debt)**\",\n",
        "        f\"• Year 1:  {_fmt(y1['cashflow'])}\",\n",
        "        f\"• Year 5:  {_fmt(y5['cashflow'])}\",\n",
        "        f\"• Year 10: {_fmt(y10['cashflow'])}\",\n",
        "        \"\",\n",
        "        f\"**Net Return (incl. Y0 outlay & cumulative cashflows)**\",\n",
        "        f\"• Year 1:  {_fmt(y1['net_return_incl_y0'])}\",\n",
        "        f\"• Year 5:  {_fmt(y5['net_return_incl_y0'])}\",\n",
        "        f\"• Year 10: {_fmt(y10['net_return_incl_y0'])}\",\n",
        "        f\"_Paper view (equity + this year's cashflow): Y1 {_fmt(y1['paper_total'])}, Y5 {_fmt(y5['paper_total'])}, Y10 {_fmt(y10['paper_total'])}_\",\n",
        "        \"\",\n",
        "        f\"• Discount rate used for NPV calculation: {pct(discount_rate)}\",\n",
        "        f\"**NPV over {years} years** (terminal sale net of 7% costs at Year {years}): {_fmt(npv_30y)}\",\n",
        "        \"\",\n",
        "        \"**Assumptions**\",\n",
        "        assumptions\n",
        "    ]\n",
        "\n",
        "    return \"\\n\".join(summary)\n",
        "\n",
        "print(\"✅ Tool 3.8 `investment_npv` loaded (robust JSON parse, address canonicalization, inputs source echo, safe-rate discount).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSiCHu5aq00b",
        "outputId": "f1eefb0c-aee2-443c-a11c-c0c536e2ba7e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tool 3.8 `investment_npv` loaded (robust JSON parse, address canonicalization, inputs source echo, safe-rate discount).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3.9: Quick Property Discovery (tool) — scrape → kNN → cash ROI\n",
        "# Depends on your existing UDFs:\n",
        "#  - scrape_redfin_to_df(...)\n",
        "#  - _get_superset_cached_df(...)   (optional; falls back if missing)\n",
        "#\n",
        "# Notes:\n",
        "# - LLM parsing inside scrape_redfin_to_df should use llama-3.3-70b-versatile (your default).\n",
        "# - We only need price & basic specs (sq ft, beds, baths). HOA/taxes not used here.\n",
        "# - Exposes globals: dfmain (rent comps), homes_for_sale_df_final (sale set).\n",
        "\n",
        "from typing import List, Dict, Any, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "# sklearn imports\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Globals / caches (safe init)\n",
        "# -----------------------------------------------------------------------------\n",
        "try:\n",
        "    DISCOVERY_RESULTS\n",
        "except NameError:\n",
        "    DISCOVERY_RESULTS: List[Dict[str, Any]] = []\n",
        "\n",
        "def _fmt_money(x):\n",
        "    try: return f\"${float(x):,.0f}\"\n",
        "    except: return str(x)\n",
        "\n",
        "def _normalize_listing_df(df: pd.DataFrame, mode: str) -> pd.DataFrame:\n",
        "    \"\"\"Unify column names & numerics; drop dupes by address; basic sanity filters.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # unify sqft column\n",
        "    if \"sq ft\" not in df.columns and \"sqft\" in df.columns:\n",
        "        df[\"sq ft\"] = df[\"sqft\"]\n",
        "\n",
        "    # numeric coercion\n",
        "    numeric_cols = [\"sq ft\", \"bedrooms\", \"bathrooms\"] + ([\"rent\"] if mode == \"rent\" else [\"price\"])\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    # drop rows with missing core fields\n",
        "    need = [\"sq ft\", \"bedrooms\", \"bathrooms\"] + ([\"rent\"] if mode == \"rent\" else [\"price\"])\n",
        "    df = df.dropna(subset=[c for c in need if c in df.columns])\n",
        "\n",
        "    # positive sanity\n",
        "    if \"sq ft\" in df.columns: df = df[df[\"sq ft\"] > 0]\n",
        "    if \"rent\" in df.columns:  df = df[df[\"rent\"] > 0]\n",
        "    if \"price\" in df.columns: df = df[df[\"price\"] > 0]\n",
        "\n",
        "    # de-dupe by address if present\n",
        "    if \"address\" in df.columns:\n",
        "        df = df.drop_duplicates(subset=[\"address\"])\n",
        "\n",
        "    return df\n",
        "\n",
        "def _scrape_sale_with_superset(\n",
        "    zip_code: str,\n",
        "    min_price_k: int,\n",
        "    max_price_k: int,\n",
        "    max_pages_sale: int,\n",
        "    cache_ttl_seconds: Optional[int],\n",
        "):\n",
        "    \"\"\"Try 3.4's superset cache first; fall back to direct scrape.\"\"\"\n",
        "    final_df = None\n",
        "    try:\n",
        "        final_df = _get_superset_cached_df(\n",
        "            mode=\"sale\",\n",
        "            zip_code=zip_code,\n",
        "            min_price_k=min_price_k,\n",
        "            max_price_k=max_price_k,\n",
        "            max_pages_to_scrape=max_pages_sale,\n",
        "            cache_ttl_seconds=cache_ttl_seconds,\n",
        "        )\n",
        "    except NameError:\n",
        "        pass\n",
        "\n",
        "    if final_df is None:\n",
        "        final_df = scrape_redfin_to_df(\n",
        "            mode=\"sale\",\n",
        "            zip_code=zip_code,\n",
        "            min_price=min_price_k,\n",
        "            max_price=max_price_k,\n",
        "            max_pages_to_scrape=max_pages_sale,\n",
        "            cache_ttl_seconds=cache_ttl_seconds,\n",
        "        )\n",
        "    return final_df\n",
        "\n",
        "@tool\n",
        "def discover_candidates_knn(\n",
        "    zip_code: str,\n",
        "    min_price_k: int,\n",
        "    max_price_k: int,\n",
        "    max_pages_sale: int = 4,\n",
        "    max_pages_rent: int = 4,\n",
        "    top_n: int = 10,\n",
        "    # optional coarse filters for the SALE pool (big-picture prefilter)\n",
        "    min_beds: int | None = None,\n",
        "    min_baths: float | None = None,\n",
        "    min_sqft: int | None = None,\n",
        "    cache_ttl_seconds: Optional[int] = 60 * 3600\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Big-picture discovery: find 5–10 interesting for-sale properties in a ZIP.\n",
        "    Steps:\n",
        "      1) Scrape rentals (train KNN on rent = f(sqft, beds, baths)).\n",
        "      2) Scrape for-sale homes (within price bounds).\n",
        "      3) Predict rent for each for-sale home; compute monthly cash ROI = rent / price.\n",
        "      4) Return top-N by cash ROI (desc). Also saves a snapshot in DISCOVERY_RESULTS.\n",
        "\n",
        "    Inputs:\n",
        "      - zip_code: e.g., \"78613\"\n",
        "      - min_price_k, max_price_k: integers in thousands (e.g., 300, 700)\n",
        "      - Optional: min_beds, min_baths, min_sqft to prefilter the SALE pool.\n",
        "    \"\"\"\n",
        "    # -------------------------------\n",
        "    # 0) Scrape data (rent & sale)\n",
        "    # -------------------------------\n",
        "    print(f\"[3.9] RENT scrape for {zip_code} (pages={max_pages_rent}) …\")\n",
        "    rent_df_raw = scrape_redfin_to_df(\n",
        "        mode=\"rent\",\n",
        "        zip_code=zip_code,\n",
        "        max_pages_to_scrape=max_pages_rent,\n",
        "        cache_ttl_seconds=cache_ttl_seconds,\n",
        "    )\n",
        "    rent_df = _normalize_listing_df(rent_df_raw, mode=\"rent\")\n",
        "    if rent_df.empty:\n",
        "        return f\"Couldn’t get usable rental comps for {zip_code}. Try increasing pages or a different ZIP.\"\n",
        "\n",
        "    print(f\"[3.9] SALE scrape for {zip_code} ${min_price_k}k–${max_price_k}k (pages={max_pages_sale}) …\")\n",
        "    sale_df_raw = _scrape_sale_with_superset(\n",
        "        zip_code, min_price_k, max_price_k, max_pages_sale, cache_ttl_seconds\n",
        "    )\n",
        "    sale_df = _normalize_listing_df(sale_df_raw, mode=\"sale\")\n",
        "    if sale_df.empty:\n",
        "        return f\"Couldn’t get for-sale listings in {zip_code} between ${min_price_k}k and ${max_price_k}k.\"\n",
        "\n",
        "    # Optional coarse filters on SALE pool\n",
        "    if min_beds is not None and \"bedrooms\" in sale_df.columns:\n",
        "        sale_df = sale_df[sale_df[\"bedrooms\"] >= int(min_beds)]\n",
        "    if min_baths is not None and \"bathrooms\" in sale_df.columns:\n",
        "        sale_df = sale_df[sale_df[\"bathrooms\"] >= float(min_baths)]\n",
        "    if min_sqft is not None and \"sq ft\" in sale_df.columns:\n",
        "        sale_df = sale_df[sale_df[\"sq ft\"] >= int(min_sqft)]\n",
        "\n",
        "    if sale_df.empty:\n",
        "        return \"After applying the sale filters, there are no homes left to score. Try relaxing filters.\"\n",
        "\n",
        "    # Expose cleaned tables globally for follow-on use (per your UX)\n",
        "    globals()[\"dfmain\"] = rent_df.copy()\n",
        "    globals()[\"homes_for_sale_df_final\"] = sale_df.copy()\n",
        "\n",
        "    # -------------------------------\n",
        "    # 1) Prepare training data (rent comps)\n",
        "    # -------------------------------\n",
        "    X = rent_df[[\"sq ft\", \"bedrooms\", \"bathrooms\"]]\n",
        "    y = rent_df[\"rent\"].astype(float)\n",
        "\n",
        "    if len(X) < 20:\n",
        "        print(f\"[3.9] Warning: only {len(X)} rental rows; KNN may be noisy.\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.30, random_state=42\n",
        "    )\n",
        "    if len(X_train) < 3:\n",
        "        return \"Training set too small for KNN/CV. Need more rental comps.\"\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[(\"num\", StandardScaler(), [\"sq ft\", \"bedrooms\", \"bathrooms\"])]\n",
        "    )\n",
        "    X_train_scaled = preprocessor.fit_transform(X_train)\n",
        "    X_test_scaled  = preprocessor.transform(X_test)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 2) Hyperparameter search (KNN)\n",
        "    # -------------------------------\n",
        "    cv_folds = max(2, min(10, len(X_train)))\n",
        "    # cap K by fold size & a hard upper bound\n",
        "    k_by_fold  = max(1, len(X_train) // cv_folds)\n",
        "    k_upper    = max(1, min(30, k_by_fold + 1))\n",
        "\n",
        "    param_grid = {\n",
        "        \"n_neighbors\": list(range(1, k_upper + 1)),\n",
        "        \"weights\": [\"uniform\", \"distance\"],\n",
        "        \"metric\": [\"euclidean\", \"manhattan\", \"chebyshev\"],\n",
        "    }\n",
        "\n",
        "    grid = GridSearchCV(\n",
        "        estimator=KNeighborsRegressor(),\n",
        "        param_grid=param_grid,\n",
        "        scoring=\"neg_mean_squared_error\",\n",
        "        cv=cv_folds,\n",
        "        n_jobs=None\n",
        "    )\n",
        "    grid.fit(X_train_scaled, y_train)\n",
        "    best_knn = grid.best_estimator_\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = best_knn.predict(X_test_scaled)\n",
        "    mse = float(mean_squared_error(y_test, y_pred))\n",
        "    r2  = float(r2_score(y_test, y_pred))\n",
        "    print(f\"[3.9] Best KNN: {best_knn.get_params()} | MSE: {mse:,.2f} | R²: {r2:,.3f} \"\n",
        "          f\"| Train n={len(X_train)}, Test n={len(X_test)}, CV={cv_folds}\")\n",
        "\n",
        "    # -------------------------------\n",
        "    # 3) Inference on for-sale homes\n",
        "    # -------------------------------\n",
        "    X_new = sale_df[[\"sq ft\", \"bedrooms\", \"bathrooms\"]]\n",
        "    X_new_scaled = preprocessor.transform(X_new)\n",
        "    rent_pred = best_knn.predict(X_new_scaled)\n",
        "    sale_df[\"predicted_rent\"] = np.maximum(0, rent_pred).astype(float)\n",
        "\n",
        "    # Cash ROI (monthly gross yield, %)\n",
        "    sale_df[\"cash_roi\"] = (sale_df[\"predicted_rent\"] / sale_df[\"price\"]) * 100.0\n",
        "\n",
        "    # Rank & pick top candidates\n",
        "    cols_out = [\"address\", \"price\", \"sq ft\", \"bedrooms\", \"bathrooms\", \"predicted_rent\", \"cash_roi\"]\n",
        "    df_ranked = (\n",
        "        sale_df.loc[:, cols_out]\n",
        "        .sort_values(\"cash_roi\", ascending=False)\n",
        "        .head(int(top_n))\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    # -------------------------------\n",
        "    # 4) Snapshot & return\n",
        "    # -------------------------------\n",
        "    result_records = df_ranked.to_dict(orient=\"records\")\n",
        "    snapshot = {\n",
        "        \"tool\": \"3.9_property_discovery_knn\",\n",
        "        \"zip_code\": zip_code,\n",
        "        \"price_bounds_k\": [int(min_price_k), int(max_price_k)],\n",
        "        \"filters\": {\"min_beds\": min_beds, \"min_baths\": min_baths, \"min_sqft\": min_sqft},\n",
        "        \"model\": {\n",
        "            \"estimator\": \"KNeighborsRegressor\",\n",
        "            \"best_params\": best_knn.get_params(),\n",
        "            \"cv_folds\": cv_folds,\n",
        "            \"test_mse\": mse,\n",
        "            \"test_r2\": r2,\n",
        "            \"train_rows\": int(len(X_train)),\n",
        "            \"test_rows\": int(len(X_test))\n",
        "        },\n",
        "        \"inputs\": {\n",
        "            \"rent_comps_rows\": int(len(rent_df)),\n",
        "            \"sale_candidates_rows\": int(len(sale_df))\n",
        "        },\n",
        "        \"results\": result_records,\n",
        "        \"timestamp\": pd.Timestamp.utcnow().isoformat()\n",
        "    }\n",
        "    DISCOVERY_RESULTS.append(snapshot)\n",
        "    print(f\"[3.9] Saved discovery snapshot. Total runs cached: {len(DISCOVERY_RESULTS)}\")\n",
        "\n",
        "    # Human-friendly + machine-friendly output\n",
        "    lines = []\n",
        "    lines.append(f\"🔎 **Top {len(df_ranked)} candidates in {zip_code} by monthly cash ROI** \"\n",
        "                 f\"(predicted_rent / price × 100) — price range ${min_price_k}k–${max_price_k}k\")\n",
        "    lines.append(f\"_KNN test MSE: {mse:,.0f}  •  R²: {r2:,.3f}  •  CV folds: {cv_folds}_\\n\")\n",
        "    for i, r in enumerate(result_records, 1):\n",
        "        lines.append(\n",
        "            f\"{i:>2}. {r['address']}\\n\"\n",
        "            f\"    Price: {_fmt_money(r['price'])} | {int(r['sq ft'])} sq ft | \"\n",
        "            f\"{int(r['bedrooms'])} bd / {int(r['bathrooms'])} ba\\n\"\n",
        "            f\"    Pred Rent: {_fmt_money(r['predicted_rent'])} | Cash ROI: {r['cash_roi']:.2f}%\"\n",
        "        )\n",
        "\n",
        "    # JSON payload for programmatic follow-ups\n",
        "    lines.append(\"\\n```json\")\n",
        "    lines.append(pd.Series({\"results\": result_records}).to_json())\n",
        "    lines.append(\"```\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "print(\"✅ Tool 3.9 `discover_candidates_knn` loaded (scrape→KNN→ROI, tool-callable).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjoVlFsJCqiU",
        "outputId": "3c74a2f2-533f-44d8-fbed-fe94dd4682f1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Tool 3.9 `discover_candidates_knn` loaded (scrape→KNN→ROI, tool-callable).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "x1CGPxy-m5J3"
      },
      "outputs": [],
      "source": [
        "# Cell 3.FINAL: Create a list of all the tools our agent can use.\n",
        "tools = [get_zipcode_stats, end_conversation, list_recently_sold_homes, list_homes_for_sale, list_homes_for_rent, competitive_market_assessment, web_intelligence_real_estate, investment_npv, discover_candidates_knn]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxdtDs03rdsC",
        "outputId": "d097a979-87d0-4fcb-cc86-13c85d76f43e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Agent persona defined.\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Define agent persona\n",
        "\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "agent_persona = \"\"\"\n",
        "You are GRACIE, an AI Realtor Assistant.\n",
        "You provide market stats, list properties, and explain real-estate trends in a friendly, conversational tone.\n",
        "You work best with zip codes. If a user provides a city and State, confirm with user the precise zip code. If user unsure then use your knowledge to convert city and state to the most dense zip code.\n",
        "CRITICAL: If the user needs to do a Competitive market assessment (CMA) and you are unsure of the price band from previous chats, you MUST ask the user for the target competitive price band BEFORE calling the CMA tool.\n",
        "Use web_intelligence_real_estate only when the user’s question cannot be answered by the other tools (stats, listings, rents, CMA) or when the user asks for definitions, policies, local facts, or citations not available from those tools (e.g., HOA details, school assignments, crime/livability, taxes/insurance, permitting/zoning, “what is a 1031 exchange”). Do not guess—if unsure which tool applies, ask one clarifying question, then call the tool.\n",
        "CRITICAL: You must use investment_npv EITHER after a CMA is complete OR YOU MUST ASK THE USER TO PROVIDE YOU WITH COMPLETE ADDRESS, ESTIMATED RENT and ESTIMATED PURCHASE PRICE.\n",
        "If the user asks for good investment candidates in a city, you must ask the user for a target zip code before calling the discover_candidates_knn tool. CRITICAL - in addition to the zip code you MUST ASK THE USER FOR INVESTMENT PRICE RANGE such as between 400K - 600K.\n",
        "You have access to the following tools:\n",
        "1. get_zipcode_stats: for general questions about a specific zipcode or city\n",
        "  Example:\n",
        "  User: “What is the median sale price in 75034?”\n",
        "  Assistant: <function=get_zipcode_stats>{\"zip_code\":\"75034\", \"property_type\":\"All Residential\"}</function>\n",
        "2. list_recently_sold_homes: for a list of recently sold homes or for any stats around homes sold.\n",
        "  Example:\n",
        "  User: “List some homes sold in 75034 between price 400K and 300K.”\n",
        "  Assistant: <function=list_recently_sold_homes>{\"zip_code\":\"75034\",\"min_price\":400000,\"max_price\":500000}</function>\n",
        "3. list_homes_for_sale: Scrapes Redfin.com for a list of homes for sale within a given ZIP code and price range. When using this tool, you must provide the full address including city and state and all other relevant information. CRITICAL: before calling this tool, ask the user for the price band.\n",
        "  Example:\n",
        "  User: “List homes for sale in 75034 between $400k and $500k.”\n",
        "  Assistant: <function=list_homes_for_sale>{\"zip_code\":\"75034\",\"min_price\":400,\"max_price\":500}</function>\n",
        "4. list_homes_for_rent: Scrapes Redfin.com for a list of single-family homes, condos, and townhouses for rent in a given ZIP code. When using this tool, you must provide the full address including city and state and all other relevant information.\n",
        "  Example:\n",
        "  User: “List homes for rent in 75034.”\n",
        "  Assistant: <function=list_homes_for_rent>{\"zip_code\":\"75034\"}</function>\n",
        "5. competitive_market_assessment: Geocode the subject; scrape cached rentals + for-sale; geocode comps; pick nearest N; estimate rent & sale price with Groq using compact prompts. When using this tool, you must use the full address including bedrooms, bathrooms, square footage, estimated price range. If beds/baths/sqft missing/price range → ask once for each missing field. Do NOT proceed if the user does not provide with you the information you need.\n",
        "  Example:\n",
        "  User: \"should I invest in 123 Main Street, Frisco 75035. It has 3 bedrooms, 2.5 baths, and 1500 sq ft.\"\n",
        "  Assistant: <function=competitive_market_assessment>{\"address\":\"123 Main Street, Frisco, TX 75035\",\"subject_bedrooms\":3,\"subject_bathrooms\":2.5,\"subject_sqft\":1500}</function>\n",
        "6. web_intelligence_real_estate: Uses Groq 'compound-beta' to answer *real-estate* questions with cited sources.\n",
        "  Example:\n",
        "  User: \"what is a 1031 exchange?\"\n",
        "  Assistant: <function=web_intelligence_real_estate>{\"query\":\"what is a 1031 exchange?\"}</function>\n",
        "7. investment_npv: Compute 30-year NPV and Year 0/1/5/10 cashflow + total return (cashflow + equity).\n",
        "  Example:\n",
        "  User: \"How profitable is 123 Main Street, Frisco 75035. It has 3 bedrooms, 2.5 baths, and 1500 sq ft?\"\n",
        "  Assistant: <function=investment_npv>{\"address\":\"123 Main Street, Frisco,75035\"}</function>\n",
        "8. discover_candidates_knn: Provides potential investment opportunities in a target zip code by Scraping Redfin.com for a list of homes for sale within a given ZIP code and price range. When using this tool, you must provide target zip code and investment range (e.g., between 400K - 600K)\n",
        "  Example:\n",
        "  User: \"Can you find me attractive investment opportunities in 75035?\"\n",
        "  Assistant: <function=discover_candidates_knn>{\"zip_code\":\"75035\",\"min_price_k\":400,\"max_price_k\":600}</function>\n",
        "9. end_conversation: Call this specific tool when the user indicates they are finished with the conversation. Use this for phrases like 'thank you', 'that's all', 'I'm done', 'thanks for the help', etc.\n",
        "  Example:\n",
        "  User: “I'm done.”\n",
        "  Assistant: <function=end_conversation>{}</function>\n",
        "\n",
        "\n",
        "When interacting:\n",
        "- CRITICAL RULE: YOU MUST ALWAYS USE THE TOOL WHEN REQUIRED. IF YOU CANNOT USE THE TOOL DO NOT MAKE UP FACTS. DO NOT HALLUCINATE.\n",
        "- CRITICAL RULE: YOU MUST ALWAYS PROVIDE SUPPORTING DATA IN ADDITION TO YOUR RESPONSE. This builds trust with the user.\n",
        "- CRITICAL RULE: After you answer a question, you must have a follow up question. Your follow up question must be limited to the capabilities the tools provide.\n",
        "- Acceptable Follow up question example 1: if the user asks for stats on zipcode 75024, acceptable follow up questions are \"would you like to see some homes for sale in 75024?\" or \"would you like to see some homes for rent in 75024?\"\n",
        "- Acceptable Follow up question example 2: after you have provided a list of homes for sale to a user, acceptable follow up questions are \"would you like to run some competitive assessment for any property you like?\"\n",
        "- Unacceptable Follow up question example: After you have provided competitive assessment do not say \"would you like to see pictures?\", since you dont have access to any tool that can fetch pictures.\n",
        "- Ask for clarification on ambiguious requests.\n",
        "- End gracefully when the user signals end of conversation with no follow up question.\n",
        "- Keep everything seamless—no mentions of “tools” or “APIs.”\n",
        "- CRITICAL: do not answer any questions unrelate\n",
        "\"\"\"\n",
        "\n",
        "system_message = SystemMessage(content=agent_persona)\n",
        "\n",
        "print(\"✅ Agent persona defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "q3k-boHbjtmb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "34bdb58c-9f3b-44af-97e2-8097b583efb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Cell 5: Define the state for our graph. It will be a list of messages.\\nclass AgentState(TypedDict):\\n    messages: Annotated[Sequence[BaseMessage], operator.add]\\n\\n# Initialize the Groq language model\\nmodel = ChatGroq(temperature=0.6, model_name=\"openai/gpt-oss-120b\", reasoning_effort=\"high\",groq_api_key=groq_api_key) #llama-3.3-70b-versatile ; llama3-70b-8192; reasoning_effort=\"medium\"\\n\\n# Bind the tools to the model. This allows the model to \"see\" the tools and know how to call them.\\nmodel_with_tools = model.bind_tools(tools)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "'''\n",
        "# Cell 5: Define the state for our graph. It will be a list of messages.\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "\n",
        "# Initialize the Groq language model\n",
        "model = ChatGroq(temperature=0.6, model_name=\"openai/gpt-oss-120b\", reasoning_effort=\"high\",groq_api_key=groq_api_key) #llama-3.3-70b-versatile ; llama3-70b-8192; reasoning_effort=\"medium\"\n",
        "\n",
        "# Bind the tools to the model. This allows the model to \"see\" the tools and know how to call them.\n",
        "model_with_tools = model.bind_tools(tools)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Define the state & model\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
        "\n",
        "# Initialize the Groq language model\n",
        "model = ChatGroq(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    temperature=0.6,                  # Groq’s recommended balanced default in 0–2 range\n",
        "    reasoning_effort = \"high\",  # turn on deeper reasoning\n",
        "    groq_api_key=groq_api_key,\n",
        ")\n",
        "\n",
        "# Bind tools once\n",
        "model_with_tools = model.bind_tools(tools)\n"
      ],
      "metadata": {
        "id": "evr8pgdsJO3L"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Build the graph with smart continuation logic\n",
        "\n",
        "import json\n",
        "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage, ToolMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "def _compact_history(messages, keep_turns: int = 10, keep_tool_messages: int = 3):\n",
        "    \"\"\"\n",
        "    Keep:\n",
        "      - the latest SystemMessage (persona),\n",
        "      - the last `keep_tool_messages` ToolMessage(s),\n",
        "      - the last `keep_turns` conversation turns (user/assistant).\n",
        "    This preserves recent context and fresh tool outputs without ballooning tokens.\n",
        "    \"\"\"\n",
        "    # Latest persona\n",
        "    system_kept = [m for m in messages if isinstance(m, SystemMessage)][-1:]\n",
        "\n",
        "    # Most recent tool results\n",
        "    tool_msgs = [m for m in messages if isinstance(m, ToolMessage)]\n",
        "    tool_kept = tool_msgs[-keep_tool_messages:] if keep_tool_messages > 0 else []\n",
        "\n",
        "    # Recent conversation messages (exclude system/tool)\n",
        "    convo = [m for m in messages if not isinstance(m, (SystemMessage, ToolMessage))]\n",
        "    convo_tail = convo[-keep_turns:]\n",
        "\n",
        "    return system_kept + tool_kept + convo_tail\n",
        "\n",
        "\n",
        "# 1) Node that calls the LLM (and may emit a tool call)\n",
        "def call_model(state: AgentState):\n",
        "    last_msg = state[\"messages\"][-1]\n",
        "\n",
        "    # After a tool runs, surface its output once and STOP (single bounce design).\n",
        "    if isinstance(last_msg, ToolMessage):\n",
        "        txt = (last_msg.content or \"\").strip()\n",
        "        return {\"messages\": [AIMessage(\n",
        "            content=txt,\n",
        "            additional_kwargs={\"from_tool_surface\": True}  # tag it so router ends the turn\n",
        "        )]}\n",
        "\n",
        "    # Otherwise proceed normally (use your wider window)\n",
        "    messages = _compact_history(state[\"messages\"], keep_turns=10, keep_tool_messages=3)\n",
        "    response = model_with_tools.invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "# 2) Node that executes a tool if the LLM requested one\n",
        "def call_tool_node(state: AgentState):\n",
        "    last_message = state[\"messages\"][-1]\n",
        "\n",
        "    # If the model didn't request a tool, we're done.\n",
        "    if not getattr(last_message, \"tool_calls\", None):\n",
        "        return {}\n",
        "\n",
        "    tool_msgs = []\n",
        "\n",
        "    for tc in last_message.tool_calls:\n",
        "        # Accept both OpenAI/Groq styles: {\"name\",\"args\"} OR {\"function\":{\"name\",\"arguments\"}}\n",
        "        fn_block = tc.get(\"function\") or {}\n",
        "        tool_name = tc.get(\"name\") or fn_block.get(\"name\")\n",
        "\n",
        "        raw_args = tc.get(\"args\")\n",
        "        if raw_args is None:\n",
        "            raw_args = fn_block.get(\"arguments\")\n",
        "\n",
        "        # Normalize arguments to a dict (model may send JSON string)\n",
        "        if isinstance(raw_args, str):\n",
        "            try:\n",
        "                tool_args = json.loads(raw_args) if raw_args.strip() else {}\n",
        "            except Exception:\n",
        "                tool_args = {}\n",
        "        elif isinstance(raw_args, dict):\n",
        "            tool_args = raw_args\n",
        "        else:\n",
        "            tool_args = {}\n",
        "\n",
        "        tool_id = tc.get(\"id\") or tc.get(\"tool_call_id\") or f\"{tool_name or 'unknown'}_call\"\n",
        "\n",
        "        if not tool_name:\n",
        "            tool_msgs.append(\n",
        "                ToolMessage(\n",
        "                    content=\"ERROR: Tool name missing in tool call.\",\n",
        "                    tool_call_id=tool_id,\n",
        "                    name=\"__tool_error__\",\n",
        "                    additional_kwargs={\"name\": \"__tool_error__\"},\n",
        "                )\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # Find the tool by name (assumes `tools` is defined globally)\n",
        "        tool = next((t for t in tools if getattr(t, \"name\", None) == tool_name), None)\n",
        "        if tool is None:\n",
        "            tool_msgs.append(\n",
        "                ToolMessage(\n",
        "                    content=f\"ERROR: Tool '{tool_name}' not found.\",\n",
        "                    tool_call_id=tool_id,\n",
        "                    name=tool_name,\n",
        "                    additional_kwargs={\"name\": tool_name},\n",
        "                )\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # Execute the tool safely\n",
        "        try:\n",
        "            result = tool.invoke(tool_args or {})\n",
        "            content = str(result)\n",
        "        except Exception as e:\n",
        "            content = f\"ERROR while running '{tool_name}': {e}\"\n",
        "\n",
        "        # Include the tool name (helps certain backends route correctly)\n",
        "        tool_msgs.append(\n",
        "            ToolMessage(\n",
        "                content=content,\n",
        "                tool_call_id=tool_id,\n",
        "                name=tool_name,\n",
        "                additional_kwargs={\"name\": tool_name},\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return {\"messages\": tool_msgs}\n",
        "\n",
        "\n",
        "# 3) Smart continuation function (single bounce after tool result)\n",
        "def should_continue(state: AgentState) -> str:\n",
        "    last_msg = state[\"messages\"][-1]\n",
        "\n",
        "    # If we just surfaced a tool result, end the turn immediately.\n",
        "    if isinstance(last_msg, AIMessage) and getattr(last_msg, \"additional_kwargs\", {}).get(\"from_tool_surface\"):\n",
        "        return \"end\"\n",
        "\n",
        "    # If the model has requested a tool, go execute it.\n",
        "    if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n",
        "        return \"continue\"\n",
        "\n",
        "    # If the last thing is a tool result (pre-surface), bounce back once.\n",
        "    if isinstance(last_msg, ToolMessage):\n",
        "        return \"continue\"\n",
        "\n",
        "    # Otherwise stop and wait for the user.\n",
        "    return \"end\"\n",
        "\n",
        "\n",
        "# ---- Build & compile the graph ----\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"tool\", call_tool_node)\n",
        "\n",
        "workflow.set_entry_point(\"agent\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\"continue\": \"tool\", \"end\": END},\n",
        ")\n",
        "\n",
        "# Always loop back after tool → agent (so the agent can surface the tool output once)\n",
        "workflow.add_edge(\"tool\", \"agent\")\n",
        "\n",
        "app = workflow.compile()\n",
        "\n"
      ],
      "metadata": {
        "id": "0NkU-5vdOTic"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: run interactive chat\n",
        "\n",
        "def run_interactive_chat():\n",
        "    \"\"\"\n",
        "    Initiates a stateful, interactive chat session with the realtor agent.\n",
        "    The agent will remember context, follow its persona, and detect when to end the conversation.\n",
        "    \"\"\"\n",
        "    disclaimer = (\n",
        "        \"⚠️  Important disclaimer\\n\"\n",
        "        \"GRACIE provides information for educational purposes only and does not constitute\\n\"\n",
        "        \"legal, tax, financial, or real estate advice. Always consult licensed professionals —\\n\"\n",
        "        \"a realtor/agent, attorney, CPA/tax advisor, lender, and/or insurance professional —\\n\"\n",
        "        \"before making decisions. Data and estimates may be incomplete or out of date.\\n\"\n",
        "        \"By continuing to use GRACIE, you explicitly consent to this disclaimer.\"\n",
        "    )\n",
        "    print(disclaimer)\n",
        "    print(\"-\" * 50)\n",
        "    print(\"🤖 Hello! I am GRACIE your AI Realtor Assistant.\")\n",
        "    print(\"I can help with real estate stats and investment opportunities.\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Initialize conversation history with the system prompt\n",
        "    conversation_history = [system_message]\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        conversation_history.append(HumanMessage(content=user_input))\n",
        "\n",
        "        inputs = {\"messages\": conversation_history}\n",
        "        # Invoke the agent graph (recursion_limit raised to 10 to avoid premature cutoff)\n",
        "        result = app.invoke(inputs, {\"recursion_limit\": 10})\n",
        "\n",
        "        # --- ROBUST CHECK FOR CONVERSATION END ---\n",
        "        conversation_over = False\n",
        "        for message in result[\"messages\"]:\n",
        "            if hasattr(message, \"tool_calls\") and message.tool_calls:\n",
        "                for tool_call in message.tool_calls:\n",
        "                    if tool_call[\"name\"] == \"end_conversation\":\n",
        "                        print(\"Ending Conversation\")\n",
        "                        conversation_over = True\n",
        "                        break\n",
        "            if conversation_over:\n",
        "                break\n",
        "        # --- END OF ROBUST CHECK ---\n",
        "\n",
        "        # Keep the entire updated message state (includes ToolMessage(s))\n",
        "        conversation_history = result[\"messages\"]\n",
        "\n",
        "        # The final response from the AI is the last AIMessage in the result (fallback: last message)\n",
        "        final_response_message = None\n",
        "        for msg in reversed(result[\"messages\"]):\n",
        "            if getattr(msg, \"type\", None) in (\"ai\",) or msg.__class__.__name__ == \"AIMessage\":\n",
        "                final_response_message = msg\n",
        "                break\n",
        "        if final_response_message is None:\n",
        "            final_response_message = result[\"messages\"][-1]\n",
        "\n",
        "        if getattr(final_response_message, \"content\", None):\n",
        "            print(f\"GRACIE: {final_response_message.content}\")\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Exit the loop if the end_conversation tool was called\n",
        "        if conversation_over:\n",
        "            break\n"
      ],
      "metadata": {
        "id": "ynuk11vud9Rp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Start the interactive chat session ---\n",
        "run_interactive_chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwZxNXXUuUUc",
        "outputId": "79fe9605-dfff-4db1-82b2-1c8305ed5860"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️  Important disclaimer\n",
            "GRACIE provides information for educational purposes only and does not constitute\n",
            "legal, tax, financial, or real estate advice. Always consult licensed professionals —\n",
            "a realtor/agent, attorney, CPA/tax advisor, lender, and/or insurance professional —\n",
            "before making decisions. Data and estimates may be incomplete or out of date.\n",
            "By continuing to use GRACIE, you explicitly consent to this disclaimer.\n",
            "--------------------------------------------------\n",
            "🤖 Hello! I am GRACIE your AI Realtor Assistant.\n",
            "I can help with real estate stats and investment opportunities.\n",
            "--------------------------------------------------\n",
            "You: bye\n",
            "Ending Conversation\n",
            "GRACIE: Conversation ended by user.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Next steps\n",
        "1 - adjust system prompt to create good follow up question (**done**)\n",
        "2 - from CMA, return (address, zipcode, latitude, longitude, beds, baths, sqft, rent, sell_price) tuple (**done**)\n",
        "3 - guard rail tool to limit chat to real estate convos - else hard close chat. 3 strikes and you are out (**done**)\n",
        "4 - model=\"compound-beta\" to find insurance and property tax information. Prophet for predicting growth. Use this for vargas analysis tool (**done**)\n",
        "5 - tool to create trend graphs (tool)\n",
        "6 - caching geocode results (**done**)\n",
        "7 - general chat engine (**done**)\n",
        "8 - find best investment opportunities (**done**)\n",
        "'''"
      ],
      "metadata": {
        "id": "GJrwt0o06DKL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5be0344a-e139-49f7-a23d-5d8d3a1507f2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNext steps\\n1 - adjust system prompt to create good follow up question (**done**)\\n2 - from CMA, return (address, zipcode, latitude, longitude, beds, baths, sqft, rent, sell_price) tuple (**done**)\\n3 - guard rail tool to limit chat to real estate convos - else hard close chat. 3 strikes and you are out (tool)\\n4 - model=\"compound-beta\" to find insurance and property tax information. Prophet for predicting growth. Use this for vargas analysis tool (tool)\\n5 - tool to create trend graphs (tool)\\n6 - caching geocode results (**done**)\\n7 - general chat engine (**done**)\\n8 - find best investment opportunities (tool)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}